{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Practical 1B Convert the given text to speech."
      ],
      "metadata": {
        "id": "XkAYIU15x6vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gTTS"
      ],
      "metadata": {
        "id": "janUJA-_px0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "mytext = \"Welcome to Natural Language programming\"\n",
        "language = \"en\"\n",
        "myobj = gTTS(text=mytext, lang=language, slow=False)\n",
        "myobj.save(\"myfile.mp3\")"
      ],
      "metadata": {
        "id": "ot5sdp_GUZim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 1C Convert audio file Speech to Text."
      ],
      "metadata": {
        "id": "JU0zlLZkyBnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install SpeechRecognition pydub\n"
      ],
      "metadata": {
        "id": "LeWgb5vDvYbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "filename = \"/content/best_4_less.wav\"\n",
        "r = sr.Recognizer()\n",
        "with sr.AudioFile(filename) as source:\n",
        "  audio_data = r.record(source)\n",
        "  text = r.recognize_google(audio_data)\n",
        "  print(text)"
      ],
      "metadata": {
        "id": "Xq1lkRr0sIUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 2A a. Study of various Corpus – Brown, Inaugural, Reuters, udhr with various methods like filelds, raw, words, sents, categories."
      ],
      "metadata": {
        "id": "BIiMa7QqyPdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52DAxzy6iqJh",
        "outputId": "cf65dd51-8636-46ad-d7f1-a913f2932273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "print ('File ids of brown corpus\\n',brown.fileids())\n",
        "ca01 = brown.words('ca01')\n",
        "# display first few words\n",
        "print('\\nca01 has following words:\\n',ca01)\n",
        "# total number of words in ca01\n",
        "print('\\nca01 has',len(ca01),'words')\n",
        "#categories or files\n",
        "print('\\n\\nCategories or file in brown corpus:\\n')\n",
        "print(brown.categories())\n",
        "\n",
        "print('\\n\\nStatistics for each text:\\n')\n",
        "print('AvgWordLen\\tAvgSentenceLen\\tno.ofTimesEachWordAppearsOnAvg\\t\\tFileName')\n",
        "for fileid in brown.fileids():\n",
        "  num_chars = len(brown.raw(fileid))\n",
        "  num_words = len(brown.words(fileid))\n",
        "  num_sents = len(brown.sents(fileid))\n",
        "  num_vocab = len(set([w.lower() for w in brown.words(fileid)]))\n",
        "  print (int(num_chars/num_words),'\\t\\t\\t', int(num_words/num_sents),'\\t\\t\\t', int(num_words/num_vocab),'\\t\\t\\t', fileid)"
      ],
      "metadata": {
        "id": "wbB2lJdVw_2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2B  Create and use your own corpora (plaintext, categorical)"
      ],
      "metadata": {
        "id": "FXFOUw2GylZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "MrVEAjIX1qbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab4e022-2633-4772-acfc-d6cf1286d3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "\n",
        "corpus_root = '/content'\n",
        "filelist = PlaintextCorpusReader(corpus_root, '.*')\n",
        "\n",
        "print('\\n File list: \\n')\n",
        "print(filelist.fileids())\n",
        "print(filelist.root)\n",
        "print('\\n\\nStatistics for each text:\\n')\n",
        "print('AvgWordLen\\tAvgSentenceLen\\tno.ofTimesEachWordAppearsOnAvg\\tFileName')\n",
        "\n",
        "for fileid in filelist.fileids():\n",
        "    try:\n",
        "        num_chars = len(filelist.raw(fileid))\n",
        "        num_words = len(filelist.words(fileid))\n",
        "        num_sents = len(filelist.sents(fileid))\n",
        "        num_vocab = len(set([w.lower() for w in filelist.words(fileid)]))\n",
        "\n",
        "        if num_words > 0 and num_sents > 0 and num_vocab > 0:\n",
        "            print(int(num_chars/num_words), '\\t\\t\\t', int(num_words/num_sents), '\\t\\t\\t', int(num_words/num_vocab), '\\t\\t', fileid)\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Skipping non-text file: {fileid}\")\n"
      ],
      "metadata": {
        "id": "pF9hXn_Ay7lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 2C.\tStudy Conditional frequency distributions"
      ],
      "metadata": {
        "id": "RmwYul_FhteU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('inaugural')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQs7Y2_JjePs",
        "outputId": "5456fca2-188f-41bf-f43c-8abcf178fdce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('udhr')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4KFpRyEj3ET",
        "outputId": "1d2a5177-61ac-4b0b-c610-47602dec469c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/udhr.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Process a sequence of pairs\n",
        "text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
        "pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]\n",
        "\n",
        "fd = nltk.ConditionalFreqDist(\n",
        "    (genre, word)\n",
        "    for genre in brown.categories()\n",
        "    for word in brown.words(categories=genre)\n",
        ")\n",
        "\n",
        "genre_word = [(genre, word) for genre in ['news', 'romance'] for word in brown.words(categories=genre)]\n",
        "print(len(genre_word))\n",
        "print(genre_word[:4])\n",
        "print(genre_word[-4:])\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(genre_word)\n",
        "print(cfd)\n",
        "print(cfd.conditions())\n",
        "print(cfd['news'])\n",
        "print(cfd['romance'])\n",
        "print(list(cfd['romance']))\n",
        "\n",
        "from nltk.corpus import inaugural\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (target, fileid[:4])\n",
        "    for fileid in inaugural.fileids()\n",
        "    for w in inaugural.words(fileid)\n",
        "    for target in ['america', 'citizen']\n",
        "    if w.lower().startswith(target)\n",
        ")\n",
        "\n",
        "from nltk.corpus import udhr\n",
        "\n",
        "languages = ['Chickasaw', 'English', 'German_Deutsch', 'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (lang, len(word))\n",
        "    for lang in languages\n",
        "    for word in udhr.words(lang + '-Latin1')\n",
        ")\n",
        "\n",
        "cfd.tabulate(conditions=['English', 'German_Deutsch'], samples=range(10), cumulative=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbZk4-3NjZIB",
        "outputId": "7c42259c-eb09-4dc7-c159-be60780f90f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "170576\n",
            "[('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ('news', 'Grand')]\n",
            "[('romance', 'afraid'), ('romance', 'not'), ('romance', \"''\"), ('romance', '.')]\n",
            "<ConditionalFreqDist with 2 conditions>\n",
            "['news', 'romance']\n",
            "<FreqDist with 14394 samples and 100554 outcomes>\n",
            "<FreqDist with 8452 samples and 70022 outcomes>\n",
            "[',', '.', 'the', 'and', 'to', 'a', 'of', '``', \"''\", 'was', 'I', 'in', 'he', 'had', '?', 'her', 'that', 'it', 'his', 'she', 'with', 'you', 'for', 'at', 'He', 'on', 'him', 'said', '!', '--', 'be', 'as', ';', 'have', 'but', 'not', 'would', 'She', 'The', 'out', 'were', 'up', 'all', 'from', 'could', 'me', 'like', 'been', 'so', 'there', 'they', 'one', 'about', 'my', 'an', 'or', 'is', 'this', 'It', 'them', 'if', 'into', 'But', 'And', 'down', 'when', 'back', 'no', 'what', 'did', 'their', 'do', 'by', 'only', 'your', 'thought', 'which', 'You', \"didn't\", 'then', 'just', 'little', 'time', 'too', 'get', 'who', 'got', 'before', 'know', 'over', 'man', 'because', 'more', 'never', 'way', 'now', 'went', 'we', \"I'm\", 'eyes', 'go', 'came', 'see', 'can', 'old', 'come', 'even', 'are', 'looked', 'other', 'They', 'its', 'knew', 'some', 'much', 'around', 'any', 'There', 'here', 'long', 'than', 'good', 'away', 'felt', 'day', 'own', 'still', 'made', 'take', \"don't\", 'say', 'going', 'how', 'something', 'after', 'through', ':', 'off', 'think', 'In', 'right', 'night', 'where', 'look', 'those', 'again', 'himself', \"I'll\", 'thing', 'first', 'might', 'seemed', 'life', 'very', 'What', \"wasn't\", 'always', 'left', 'make', 'young', 'put', 'being', 'people', 'while', 'took', 'two', 'turned', 'A', 'nothing', 'saw', 'told', 'head', \"couldn't\", 'home', 'asked', 'place', 'room', 'must', 'His', 'mother', 'face', 'wanted', 'last', 'Phil', 'door', 'next', 'will', 'against', 'anything', 'us', 'Then', 'No', 'herself', 'enough', 'morning', 'let', 'Mrs.', 'John', 'once', 'This', 'boy', 'really', 'well', 'tell', 'When', 'few', 'stood', 'want', 'looking', 'course', 'house', 'big', 'feel', 'hand', 'ever', 'woman', 'why', 'Well', 'find', 'until', 'cold', 'kind', 'water', 'years', 'voice', \"wouldn't\", 'son', 'All', 'Mr.', 'along', \"I'd\", 'black', 'gave', 'sat', 'work', 'better', 'should', 'days', 'love', 'called', 'new', 'For', 'heard', 'small', 'We', 'hands', 'these', 'without', 'same', 'white', 'hair', 'sure', 'great', 'things', 'Lucy', 'church', 'men', 'That', 'else', 'though', 'At', 'Her', 'done', 'found', \"hadn't\", 'Now', 'both', 'Just', \"It's\", 'give', 'Why', 'If', 'Miss', 'Mike', 'everything', 'many', \"I've\", 'moment', 'walked', 'myself', 'job', 'Cady', 'kept', 'girl', 'clothes', 'keep', 'has', 'world', 'another', 'most', 'baby', 'stopped', 'beautiful', 'together', 'our', 'gone', 'Yes', 'pale', 'talk', 'Linda', 'Johnnie', 'each', 'light', 'having', 'money', \"can't\", 'leave', 'thinking', 'Oh', \"Don't\", 'end', 'call', 'field', 'help', 'alone', 'mind', 'Myra', 'Theresa', 'Not', 'smiled', 'began', 'sun', 'sound', 'Anne', 'father', 'every', 'trying', 'bed', 'whole', 'family', 'idea', 'God', 'wrong', 'Wally', 'women', 'toward', 'Maggie', 'started', 'stay', 'quite', 'tried', 'wish', 'between', \"she'd\", 'suddenly', 'slowly', 'Spencer', 'Bobbie', 'Martin', 'Cousin', 'Eddie', 'Deegan', 'early', \"that's\", 'wife', '(', ')', 'yet', 'such', 'feet', 'used', 'feeling', 'My', 'business', 'weeks', 'George', 'snake', 'watching', 'seen', 'children', 'high', 'doing', \"isn't\", 'am', 'already', 'Henrietta', 'Owen', 'dark', 'Alexander', 'As', 'car', 'Susan', 'Freddy', 'since', 'happened', 'pretty', 'taking', 'care', 'coffee', 'lived', 'table', 'blue', 'body', 'sitting', 'sorry', 'turn', 'full', 'almost', 'brought', 'across', 'air', 'later', 'letter', 'Cathy', 'Dolores', 'given', 'part', 'matter', 'nice', 'fine', 'saying', 'So', \"won't\", 'open', 'week', \"That's\", 'Sam', 'ball', 'tractor', 'shorts', 'William', 'Old', 'above', 'front', 'making', 'side', 'pink', 'rest', 'lot', 'child', 'set', 'three', 'half', 'talking', 'How', 'rather', 'word', 'couple', 'true', 'clear', 'heart', 'bright', 'real', 'Nadine', 'Richard', 'stop', 'upon', 'hear', 'One', \"You're\", 'close', 'picked', 'hard', 'run', 'getting', \"he'd\", 'loved', 'hours', 'Of', 'beside', 'breakfast', 'held', 'chance', 'standing', \"you're\", 'Rachel', 'waited', 'Hanford', 'Willis', 'To', 'Or', 'past', 'dead', 'behind', 'town', 'waiting', 'also', 'perhaps', 'Eugenia', 'New', 'second', 'need', 'talked', 'red', 'caught', 'moved', 'far', 'hot', 'school', 'age', 'girls', 'bad', 'maybe', 'Poor', 'Anniston', 'evening', 'street', 'remember', 'stared', 'times', 'words', 'dropped', 'instead', 'American', 'under', 'fell', 'ought', 'use', \"'\", 'Doc', 'year', 'yourself', \"He's\", 'Japanese', 'Tommy', 'believe', 'understand', 'soon', 'listening', 'rock', 'apartment', 'Edythe', 'refrigerator', 'Man', 'name', 'With', 'fingers', 'yellow', 'color', 'Even', 'heavy', 'arms', 'lost', 'opened', 'company', 'office', 'York', 'everybody', 'laughed', 'floor', \"it's\", 'doctor', 'ran', 'live', \"haven't\", 'read', 'coming', 'note', 'On', 'certain', 'remembered', 'seeing', 'friends', 'wondered', 'swimming', 'probably', 'realized', 'Cromwell', 'large', 'boys', 'straight', 'quietly', 'Carla', 'Jim', 'liked', 'sounds', 'tiny', 'Did', 'hold', 'less', \"doesn't\", 'death', 'afternoon', 'Do', 'neck', 'sick', 'kitchen', 'bedroom', 'bit', 'brown', 'shoes', 'meeting', 'fire', 'spoke', 'best', 'hour', 'party', 'met', 'walking', 'rain', 'gray', 'short', 'move', 'green', 'Navy', 'Maybe', 'least', 'college', 'added', 'guy', 'interest', 'kill', 'late', 'Charlie', 'catcher', 'Frankie', 'six', 'tongue', 'nobody', 'thin', 'anyone', 'silent', 'steps', 'inside', 'running', 'ten', 'months', 'answer', 'within', 'eye', 'knowing', 'mouth', 'watch', 'tired', 'Chris', 'living', 'worry', 'during', 'outside', 'war', 'wore', 'meet', 'strange', 'stayed', 'dressed', 'lips', 'simply', 'lay', 'meant', 'handsome', 'sent', 'surprised', 'smile', 'able', 'someone', 'After', 'does', 'stick', 'different', 'today', 'hall', 'reason', 'may', 'finally', 'group', 'among', 'speak', 'shouted', 'ready', 'taken', 'afraid', 'colors', 'Quint', 'Pete', 'Lucille', 'Vivian', 'country', 'except', \"shouldn't\", 'known', 'city', 'low', 'walls', 'alive', 'sleep', 'case', 'leaned', 'wet', \"She's\", 'warm', 'wide', 'ones', 'seem', 'Grandma', \"Let's\", 'government', 'sort', 'forever', 'teeth', 'anyway', 'married', 'yes', 'followed', 'drove', 'line', 'hope', 'promise', 'drank', 'please', 'try', 'husband', 'wished', 'wait', 'till', 'sea', 'passed', 'clean', 'drink', 'ahead', 'cry', 'sometimes', 'mean', 'visit', 'tomorrow', 'book', 'walk', 'themselves', 'drew', 'wonder', 'needed', 'innocent', 'ask', 'Your', 'hell', 'Via', 'Paris', 'beach', 'manager', 'pitcher', 'Dave', 'tone', 'Yet', 'thick', 'present', 'Once', 'carry', 'paid', 'taste', \"man's\", 'Perhaps', 'spent', 'rose', 'window', 'beyond', 'threw', 'either', 'bring', 'whose', 'beginning', 'near', 'corner', 'hearing', 'food', 'moments', 'strong', 'changed', 'stairs', 'dress', 'settled', \"weren't\", 'supposed', 'Something', 'start', 'trip', 'person', \"she's\", 'hardly', 'class', 'others', 'imagine', 'show', 'nodded', 'beard', 'possible', 'answered', 'regular', 'jacket', 'turning', 'trouble', 'Nobody', 'minutes', 'religion', 'luck', 'game', 'longer', 'Captain', 'paper', 'knife', 'stand', 'returned', 'anger', 'Nothing', 'bench', 'ago', 'cool', 'crazy', 'hate', 'shut', \"you've\", 'decided', \"What's\", 'ugly', 'scared', 'president', 'Elec', 'Warren', 'club', 'biwa', 'Partlow', 'broken', 'sweet', 'rising', 'dog', 'smelled', 'wall', 'shop', 'Only', 'top', 'pay', 'fifteen', 'order', \"There's\", 'dream', 'quickly', 'ground', 'happy', 'shook', 'strength', 'plenty', 'Evans', 'laughing', 'working', 'truth', 'bottle', 'evil', 'burst', 'wearing', 'poor', \"you'll\", 'nearly', 'forth', 'Robards', 'Let', 'dear', 'difficult', 'change', 'marriage', 'forward', 'stomach', 'special', 'spirits', 'summer', \"We'll\", 'notice', 'Doaty', 'flowers', 'play', 'pulled', 'sense', 'mine', 'earth', 'necessary', 'sky', 'impossible', 'hospital', 'English', 'faces', 'image', 'considered', 'Buzz', 'worked', 'happen', 'fashion', 'bag', 'shirt', 'Is', 'quiet', 'guess', 'fortune', 'doubt', 'Here', 'four', 'building', 'shrugged', 'Gratt', 'somebody', 'asleep', 'picture', 'painting', 'Salter', 'Rome', 'Look', 'lodge', 'mad', 'sharp', 'phone', 'Dad', 'Cooper', \"John's\", 'Alice', 'Charlotte', 'Ryusenji', 'temple', 'Hamrick', 'neither', 'pushed', 'distance', 'touch', 'shoulders', 'climbed', 'sister', 'Italy', 'foot', 'center', 'trees', 'glass', 'porch', 'holding', 'named', 'smoke', 'older', 'leg', 'pointed', 'showed', 'lines', 'somewhat', 'shining', 'space', 'figure', 'pleasure', 'flesh', 'became', 'comfortable', 'carried', 'round', 'figures', 'minute', 'twenty', 'stock', 'fast', 'somehow', 'begged', 'hoped', 'die', 'anybody', 'Jenny', 'nose', 'weather', 'marry', 'died', 'peace', 'often', 'speaking', 'indeed', 'concern', 'worried', 'enjoyed', 'glad', 'folks', 'send', 'funny', 'safe', 'daughter', 'garden', 'duty', 'whatever', 'narrow', 'An', 'understood', 'Come', 'eating', 'arm', 'grow', 'leaving', 'letters', 'watched', 'main', 'mirror', 'says', 'wonderful', 'ship', 'Riverside', 'dinner', 'explained', 'suppose', 'desk', 'hotel', 'conversation', 'smiling', 'dollars', 'slightly', \"he's\", 'studied', 'carrying', 'entered', 'Heiser', 'further', 'seven', 'From', 'grinned', 'fight', 'hurt', 'naked', 'suit', 'whether', 'wants', 'Donald', 'By', 'Burns', 'reading', 'fog', 'length', 'busy', 'Julia', 'Julie', 'Sabella', 'Mousie', 'Ken', 'fall', 'Emma', 'Kirby', 'sweater', 'lotion', 'dugout', 'Fudo', 'College', 'leadership', 'David', 'rang', 'month', 'parents', 'orange', 'beneath', 'become', 'onto', 'direction', 'soul', 'guessed', 'memory', 'houses', 'cemetery', 'rich', 'sit', 'service', 'pocket', 'pipe', 'chest', 'sign', 'reached', 'moving', 'Suddenly', 'sight', 'skin', 'sake', 'rolled', 'slid', 'glance', 'expected', 'closer', 'Stuart', 'social', 'insisted', 'keeping', 'envelope', 'Every', 'bread', 'sighed', 'jobs', 'wondering', 'grew', 'desperate', 'remained', 'hated', 'worse', 'Lord', 'suspected', 'Tolley', \"He'd\", 'fact', 'laugh', 'none', 'write', \"You've\", 'spring', 'Dr.', 'damn', 'horse', 'horses', 'Gunny', 'Jen', 'quick', 'suggested', 'pleased', 'tears', 'unhappy', 'giving', \"you'd\", 'return', 'driving', 'shame', 'Have', 'Club', 'ideas', \"Doaty's\", 'knowledge', 'curious', 'music', 'stone', 'sad', 'growing', 'Outside', 'Go', 'sounded', 'interested', 'cottage', 'terrible', \"Can't\", 'places', 'reasons', 'Probably', 'limp', 'pick', 'defense', 'Who', 'voices', 'easy', 'grown', 'soft', 'ward', 'approach', 'stag', 'admit', 'finished', 'road', 'played', 'bottom', 'glasses', 'telling', 'raised', 'books', 'usual', 'five', 'appeared', 'blonde', 'dance', 'Somers', 'Gansevoort', 'sir', \"Spencer's\", 'missing', 'Are', 'cannot', 'coat', 'teacher', 'fighting', 'Sometimes', 'deep', 'Out', 'hit', 'kid', 'Shafer', 'wedding', 'drive', 'ham', 'truck', 'miserable', 'drunk', 'looks', 'closet', 'advertising', 'rooms', 'kids', 'studio', 'younger', 'bitter', 'attention', 'guests', 'habit', 'Was', 'cried', 'decision', 'funeral', 'bathroom', 'Pope', 'breath', 'whip', 'yours', 'future', 'wind', 'flat', 'Will', 'dolls', 'guest', 'Stop', 'Victoria', 'honest', 'Gladdy', 'cut', 'beauty', 'stake', 'Alma', 'pill', 'peaked', 'team', 'Lee', 'Rossoff', 'student', 'Tokyo', 'interests', 'science', 'Adam', 'nor', 'streets', 'missed', 'unusual', 'youth', 'Sunday', 'passing', 'Street', 'wine', 'passion', 'cared', 'park', 'playing', 'cards', 'neighborhood', 'wood', 'owned', 'whom', 'price', 'step', 'Pompeii', 'ride', 'calling', 'stuff', 'Laura', 'toes', 'mood', 'milk', 'smooth', 'brother', 'reach', 'filled', 'excitement', 'proud', 'Little', 'fat', 'developed', 'laid', \"father's\", 'bought', 'plain', 'stove', 'snap', \"let's\", 'sleeping', 'fair', 'relief', 'furnace', 'giant', 'heater', 'camp', 'worries', 'proved', 'burned', 'mentioned', 'Kizzie', 'pictures', 'simple', 'Frank', \"They're\", 'fault', 'exactly', 'lie', 'parties', 'touching', 'fool', 'animal', 'sympathy', 'humor', 'winter', 'rise', 'Those', 'box', 'friendly', 'land', 'hat', 'Adelia', 'Their', 'dancing', 'Papa', 'circle', 'question', 'birds', 'chair', 'nature', 'slipped', 'human', 'thirty', 'impulse', 'fellow', 'captain', 'size', \"there's\", 'harm', 'beat', 'beer', \"You'll\", 'led', 'fish', 'edge', 'gathering', 'principal', 'blood', 'average', 'sunburn', 'Japan', 'putting', 'San', 'patients', 'tight', 'anywhere', 'check', 'angry', 'promised', 'Actually', 'mast', 'chief', 'Base', 'telephone', 'Whitey', \"we're\", 'Parker', 'nervous', 'wildly', 'aware', 'forget', 'helped', 'lawyer', 'explain', 'easily', \"o'clock\", 'eat', 'announced', 'redhead', 'checks', 'ordered', 'Philip', 'letting', 'prisoners', 'listen', 'Three', 'rushed', 'mass', 'study', 'market', 'stepped', 'Europe', 'brilliant', 'follow', 'Reuveni', 'weight', 'difference', 'dying', 'While', 'lean', 'store', 'written', 'handle', 'eleven', 'various', 'actually', 'usually', 'paint', 'Askington', 'profession', 'national', 'twelve', 'point', 'extreme', 'Ah', 'religious', 'path', 'itself', 'friend', 'drop', 'broad', 'pair', 'aside', 'wake', 'stuck', 'nephew', 'happiness', 'stumbled', 'Walitzee', 'pain', 'shouting', 'darkness', 'hollow', 'ears', 'discovered', 'Mother', 'mention', 'Please', 'goes', 'character', 'sand', 'chosen', 'bathing', 'snapped', 'Bill', 'tonight', 'John-and-Linda', 'pool', 'Jack', 'Janice', 'Thom', 'gotten', 'tough', 'lots', 'Joe', 'carefully', 'Tuxapoka', 'Can', 'League', 'salami', 'Tom', 'plate', 'players', 'god', 'Acala', 'furrow', 'snakes', 'motivation', 'electronics', 'situation', 'A-Z', 'Zenith', 'Richert', 'disliked', 'bell', 'noon', 'breaking', 'buzzing', 'screen', 'June', 'below', 'female', 'heat', 'although', 'quarters', 'Saturday', 'hundred', 'suitcase', 'rode', 'swayed', 'Rose', 'spun', 'hatred', 'square', 'hill', 'Love', 'fear', 'coolness', 'brief', 'cane', 'remove', 'Hey', 'swung', 'interrupted', \"ain't\", \"boy's\", 'due', 'shaved', 'bear', 'flying', 'dry', 'deal', 'juice', 'washing', 'odd', \"baby's\", \"She'll\", 'nine', 'interesting', 'ashamed', 'wishes', 'appearance', 'grim', 'buildings', 'poverty', 'regarded', 'East', 'circumstances', 'bills', 'soles', 'dropping', 'cracks', 'range', 'important', 'ice', 'talent', 'Christian', 'especially', 'Hope', 'crisp', 'Laban', 'Roy', 'Zion', \"it'll\", 'favorite', 'unable', 'opening', 'pot', 'Surely', 'Quinzaine', 'poured', 'bother', 'Dunne', 'ways', 'manner', 'danger', 'curled', 'lack', 'expensive', 'showing', \"'em\", 'mighty', 'legs', 'More', 'welcome', 'nurse', 'fit', 'crowd', 'gets', 'See', 'Mama', 'Charles', 'choice', 'awake', 'forgotten', 'Everything', 'closed', 'swelling', 'bare', 'downstairs', 'Rosa', 'tied', 'danced', 'branches', 'offered', 'plan', 'straightened', 'tissues', 'island', 'blame', 'defend', 'ancient', 'bent', \"day's\", 'wild', 'covered', 'hoping', 'tourist', 'trade', 'Most', 'lovely', 'Diego', 'serious', 'States', 'involved', 'drinking', 'obviously', 'lieutenant', 'rage', 'Take', 'training', 'accepted', 'language', 'Two', 'experienced', 'America', 'male', 'terribly', 'key', 'natural', 'certainly', 'member', 'opinion', 'entirely', 'delight', 'hills', 'taxi', 'smart', \"one's\", 'Listen', 'Gertrude', 'frowned', 'lay-sisters', 'effect', 'admitted', 'miles', 'Pacific', 'plays', 'excuse', 'card', 'ate', 'Although', 'crowded', 'combination', 'won', 'champagne', 'dirty', 'arranged', 'authentic', 'Five', 'everyone', 'fourteen', \"We're\", 'broke', 'lump', 'murder', 'cabin', 'news', 'Where', 'deck', 'brace', 'sending', 'signal', 'begin', 'placed', 'joined', 'aboard', 'force', 'number', 'Besides', 'lead', 'positive', 'lose', 'dignity', 'plot', 'agreed', 'hesitate', 'guilty', 'Some', 'alley', 'trousers', 'tightly', 'swinging', 'Like', 'stupid', 'Israel', 'chose', 'nerve', 'relaxed', 'refugee', 'continued', 'skinny', 'imagined', 'gang', 'board', 'maid', 'thinks', 'tire', 'crying', 'damned', 'diamond', 'makes', 'means', 'share', 'gift', 'St.', 'courage', 'chill', 'opposite', 'sales', 'sold', 'doorway', 'painted', 'paintings', 'easel', 'admired', 'joy', 'pure', 'eighteen', 'lemon', 'Dolly', 'assumed', \"women's\", 'lunch', 'equal', 'final', 'release', 'Everyone', 'value', 'advice', 'darling', 'Father', 'arrived', 'knees', 'fountain', 'midnight', 'woke', \"mother's\", 'worrying', 'lady', 'Agnese', 'folded', 'experience', 'Early', 'Spring', 'Pile', 'Clouds', 'spat', 'dreamed', 'damp', 'staring', 'circles', 'Rawlings', 'Millie', 'slept', 'plane', 'vivid', \"aren't\", 'receiver', 'packed', \"Myra's\", 'widow', 'Sure', 'rocks', 'perfect', 'waves', 'yelled', 'cough', 'Better', \"we'll\", 'earlier', 'ruined', 'Chandler', 'steel', 'bride', 'suits', 'blessing', 'managed', 'Francie', 'possibly', 'throw', 'cars', 'position', \"we'd\", 'bureau', 'story', 'anyhow', 'Dick', 'Seven', 'Bancroft', 'Groggins', 'guys', 'sex', 'snow', 'Stubblefield', 'arched', 'Stubblefields', 'pills', 'Somebody', 'spread', 'Always', 'inning', 'bat', 'dressing', 'seated', 'Ricco', 'gonna', 'towards', 'baseball', 'Samuel', 'plow', 'killed', 'soil', 'Good', 'engineer', \"hasn't\", 'Gerry', \"Freddy's\", 'Allstates', 'Wisconsin', 'Ticonderoga', 'boredom', 'smell', 'ripe', 'wrapped', 'dogs', 'kiss', 'dollar', 'bill', 'noticed', 'Drexel', 'higher', 'thinner', 'stockings', 'miniature', 'wrinkled', 'station', 'intervals', 'companion', 'divided', 'ash', 'concrete', 'wooden', 'doors', 'thrown', 'iron', 'Above', 'mysterious', 'form', 'forked', 'gossip', 'silently', 'fed', 'devil', 'determined', 'belly', 'dreaming', 'Nor', 'fist', 'bowing', 'smoked', 'fence', 'torn', 'fold', 'sullen', 'quality', 'fully', 'chin', 'aged', 'goat', 'boiling', 'taut', 'warmth', 'roof', 'player', 'shadow', 'pass', 'raise', 'sandals', 'examining', 'belonged', 'sink', 'shrill', \"Maggie's\", 'complexion', 'sunny', 'activity', 'catch', 'lugged', 'besides', 'machine', 'dresses', 'Evadna', 'Mae', 'hang', 'careful', 'block', 'tea', 'tables', 'serve', 'type', 'happier', 'typewriter', 'whenever', 'waste', 'City', 'icy', 'grip', 'physical', 'climate', 'eternal', 'agencies', 'facilities', 'depression', 'dragging', 'West', 'meanwhile', 'theme', 'personal', 'dentist', 'minor', 'twice', 'coal', 'loosened', \"Grandma's\", 'wretched', 'mainly', 'shivering', 'upper', 'beds', 'mouthful', 'lives', 'brains', 'superior', 'dragged', \"Didn't\", \"Tolley's\", 'empty', 'kissed', 'decide', 'oh', 'Swift', 'Under', \"God's\", 'tore', 'shall', 'gather', 'envy', 'Another', 'effort', 'Mare', 'bless', 'Never', 'mark', 'racing', 'triumph', 'understanding', 'silly', 'Night', 'attentive', 'triplets', 'disappointment', 'pike', 'fetch', 'deeply', 'wear', 'thanks', 'silver', 'Kiz', 'chickens', 'babies', 'lift', 'French', 'pupils', 'gay', 'enormously', 'Blackwell', 'heavily', 'Thank', 'easier', 'moral', 'dawn', 'lying', 'ability', 'Before', 'grave', 'impatient', 'comfort', 'permitted', 'Feeling', 'yesterday', 'bouquet', 'Folly', 'impression', 'eager', 'village', 'ear', 'estimate', 'choose', 'coffin', 'system', 'sharply', 'Next', 'spoken', 'actions', 'Island', 'comes', 'visiting', 'nod', 'lifted', 'hungry', 'lively', 'fallen', 'savage', 'moonlight', 'clouds', 'stolen', 'Momoyama', 'tall', 'bridge', 'tilted', 'considering', 'request', \"doctor's\", 'suffered', 'Such', 'tours', 'Fleet', 'Yokosuka', 'Beach', 'sports', 'Hong', 'Kong', 'marked', 'applied', 'foreign', 'charm', 'date', 'officer', 'decency', 'thumb', 'muttered', 'Everybody', 'charming', 'addressed', 'Spanish', 'United', 'steak', 'sandwich', 'cup', 'remain', 'becoming', 'slight', 'cheerful', 'naturally', 'report', 'Perry', 'convinced', 'executive', 'members', 'draw', 'enjoy', 'spare', 'Pagan', 'Room', 'Friday', 'Thanks', 'rid', 'Our', 'bringing', 'freezing', 'resent', 'pitch', 'retreated', 'knocking', 'proper', 'active', 'whiskey', 'bar', 'showered', 'Golden', 'Calf', 'dice', 'test', 'management', 'dimly', 'lit', 'hopeful', 'cowboy', 'steady', 'build', 'repeating', 'Wrangler', 'spend', 'Sparky', 'loss', 'Hurrays', 'spirit', 'exhibition', 'wheel', 'boss', 'success', \"How's\", 'honey', 'hurry', 'throat', \"name's\", 'offer', 'planned', 'address', 'arrest', 'Greek', 'Somehow', 'handspikes', 'McKinley', 'reported', 'crash', 'heads', 'Small', 'violently', 'swear', 'evident', 'turmoil', 'ideal', 'battle', 'irons', 'followers', 'Wilson', 'Wales', 'useless', 'informed', 'truly', 'paused', 'stands', 'suspicious', 'lowered', 'act', 'failed', 'foolish', 'reward', 'someday', 'shake', 'guilt', 'painful', 'recognized', 'red-haired', 'shocked', 'favored', 'constant', 'favor', 'whispered', 'continue', 'frightful', 'approached', 'glancing', 'fixed', 'paot', 'sides', 'swallowed', 'windows', 'flight', 'peering', 'Yiddish', 'ghettos', 'noses', 'Since', 'worn', 'hung', 'rigid', 'rubbed', 'glanced', 'skirt', 'horrible', 'orthodox', 'amazed', 'section', 'visited', 'freedom', 'sought', 'intimate', 'pace', 'plans', 'begun', 'swing', 'Wednesday', 'nights', 'headlights', 'ceiling', 'complete', 'casual', 'waving', 'listened', 'Memphis', 'mayor', 'balloon', 'mud', 'changing', 'gives', 'Because', 'particularly', 'reflecting', 'thoughts', 'toilet', 'lock', 'encourage', 'bangs', 'break', 'drag', 'clutching', 'cooling', 'overhead', 'magazine', 'delicate', 'details', 'sheet', 'instance', 'covers', 'research', 'armor', 'McKenzie', 'art', 'third', 'cost', 'middle', 'Monmouth', 'page', 'artist', 'drawn', 'golden', 'prominent', 'slick', 'Life', 'photograph', 'library', 'gorgeous', 'studying', 'classes', 'painter', 'embrace', 'common', 'Pendleton', 'modern', 'leaders', 'ugliness', 'Tuesday', 'glove', 'Had', 'signed', 'thank', 'remark', 'weekend', 'river', 'upstairs', 'Harvie', 'disturbed', 'shudder', 'outcry', 'knows', 'absurd', 'connection', 'thousands', 'hanging', 'sacred', 'traffic', 'Again', 'Uhhu', 'wanting', 'history', 'Would', 'lower', 'snuggled', 'somewhere', 'Ferraros', 'tennis', 'Signora', 'North', 'lumber', 'boxcar', 'scornful', 'trembling', 'hideous', 'images', 'pressed', 'Tell', 'wolf', 'upright', 'nagging', 'sticks', 'noise', 'screaming', 'demons', 'Get', 'sank', 'happens', 'enter', 'brain', 'bore', 'thousand', 'whine', 'meaning', 'Move', 'hoarse', 'cave', 'single', 'grace', 'vanished', 'Hell', 'ridge', 'grotesque', 'totally', 'shape', 'committed', 'dive', 'jumped', 'Class', 'ghost', 'remote', 'bandages', 'mess', 'gentleman', 'Strange', 'cheekbones', 'leading', 'Uncle', 'ridiculous', 'general', 'crossed', 'Joan', 'win', 'Okay', \"Jim's\", \"She'd\", 'registered', 'awfully', 'grateful', 'appreciated', 'nervously', 'troubled', 'hers', 'yell', 'sharing', 'silence', 'good-by', 'cowbirds', 'solitary', \"child's\", 'row', 'surrounded', 'tension', 'bet', 'realize', 'hunting', 'buy', \"they'd\", 'tourists', 'curve', 'Quintus', 'intended', 'nightmare', 'Big', 'Mister', 'metal', 'ocean', 'shot', 'duck', \"Isn't\", 'Latin', 'waiter', 'Part', 'glared', 'skipped', 'backward', 'unbearable', 'smashed', 'birthday', 'attractive', 'rarely', 'surely', 'Longue', 'Vue', \"Linda's\", 'ceremony', \"who'd\", 'brave', 'keeps', 'generous', 'Washington', 'successful', 'save', 'stream', 'anxious', 'steels', 'Lovejoy', 'Funk', 'Shirley', \"Nadine's\", 'flushed', 'Roberts', \"Wally's\", 'dozed', 'figured', 'thankful', 'fired', 'cent', 'incredible', 'practically', 'fresh', 'Fairview', 'secret', 'Chief', 'elevator', 'shock', \"woman's\", 'scar', \"We'd\", 'dull', 'slide', 'private', 'Suppose', 'refused', 'tremble', 'acceptance', 'solution', 'deeper', 'recent', 'cognac', 'sober', 'wisdom', 'wise', 'toothbrush', 'bath', 'subject', 'plants', 'Carraway', 'scrawled', 'tense', 'agony', 'cat', 'ladies', 'rides', 'batting', 'affairs', 'driven', 'touched', 'Ronald', 'burning', 'reasonable', 'veranda', \"Kirby's\", 'flu', 'Peony', \"what's-his-name\", 'several', 'Be', 'worst', 'closely', 'wrote', 'absorbed', 'puny', 'base', 'tossed', 'ballplayers', \"Mike's\", 'dirt', \"Richard's\", 'Book', 'Dead', 'Asian', 'basement', 'flames', 'rush', 'searching', 'Fudomae', \"Charlotte's\", 'recall', 'tale', \"brother's\", 'plowed', 'jar', 'brutality', 'Snakes', 'likes', 'push', 'production', 'treasurer', 'firm', 'equipment', 'Anthea', 'expenditure', 'Cap', 'General', 'financing', 'Arthur', \"William's\", 'Herberet', 'plant', 'Ham', \"We've\", 'whirling', 'bats', 'lights', 'obeyed', 'commanded', 'confession', 'ignored', 'blind', 'Without', 'directly', 'Torino', 'Eh', 'stage', 'slapped', 'thighs', 'caressed', 'smoothness', 'rosaries', 'sweeter', 'condemned', 'fig', 'consumed', 'questioned', 'wave', 'exhausted', 'farewell', 'altered', \"Man's\", 'climbing', 'glued', 'stretched', 'roared', 'stucco', 'Sameness', 'sloping', 'upward', 'porches', 'screeched', 'housed', 'scent', 'lawn', 'hedge', 'supporting', 'facade', 'everywhere', 'blocks', 'fortresses', 'property', 'privacy', 'shaded', 'grape', 'licked', 'sinister', 'Aunt', 'provided', 'tribute', 'restless', 'Americans', 'brushing', 'drying', 'tanned', 'arch', 'swept', 'features', 'dreams', 'lapping', \"goat's\", 'sphere', 'Christ', 'jowls', 'swollen', 'teats', 'straining', 'storm', 'finger', 'reassured', 'pretended', \"Concetta's\", 'disfigured', 'People', 'possession', 'rapidly', 'huge', 'basket', 'household', \"wife's\", 'contributing', 'upkeep', 'soap', 'desperation', 'seems', 'baked', 'nursery', 'flannel', 'extra', 'list', 'assets', 'yard', 'elaborate', 'typing', 'bitterly', 'numbers', 'stark', 'cities', 'greedy', 'Western', 'sunshine', 'mountains', 'endless', 'resources', 'huddled', 'offices', 'newspaper', 'protected', 'rates', 'west', 'doubtless', 'Shivering', 'argued', 'song', 'problems', 'dozen', 'fuel', 'gas', 'fifty', 'catastrophe', 'Abernathy', 'ravenous', 'depths', 'widened', 'Presently', 'depend', 'removed', 'installed', 'freak', 'undressed', 'plunged', 'grimly', 'Plenty', 'ruin', 'prove', 'pneumonia', 'span', 'bedspread', 'trusted', 'deliberately', 'joke', \"they're\", 'Mamma', 'pin', 'Pictures', 'camera', 'Indeed', 'marrying', 'Mt.', 'Pleasant', 'Wait', 'hooked', 'Hurry', 'Fairbrothers', 'Make', 'congregation', 'cruel', 'unfair', 'rules', 'thunder', 'preserved', 'thrusting', 'burden', 'remaining', 'sons', 'necessity', 'clay', 'pieces', 'flame', 'haunting', \"Fair's\", 'graves', 'toast', 'absent', 'adding', 'Simply', 'Whipsnade', 'Mist', 'scream', 'assured', 'importance', 'furnishings', 'fail', 'spiritual', 'irritating', 'token', 'barn', 'bite', 'scarf', 'nerves', 'foal', 'laments', 'afterwards', 'robbed', 'mare', 'greatness', 'conflict', 'intensely', 'indifference', 'Tillie', 'aversion', 'chances', 'veterinarian', 'rubbing', 'cook', 'carpet', 'allowed', 'hon', 'cute', 'firmly', \"an'\", 'elegant', 'Sis', 'changes', 'rings', 'trips', 'frame', 'cats', 'Though', 'collar', 'holy', 'reserved', 'regretted', 'swell', 'dare', 'teaching', 'hop', 'hesitation', 'momentary', 'lonesome', 'relish', 'uncomfortably', 'murmured', 'loving', 'extraordinary', 'Hetty', 'tempted', 'childhood', 'School', 'taught', 'profound', 'distaste', 'tireless', 'one-two-three', 'supper', 'balls', 'sighing', 'ignore', 'regard', 'temptation', 'sin', 'creep', 'mission', 'sentimental', 'enclosed', 'grass', 'dew', 'bird', 'sang', 'engine', 'barefoot', 'neat', 'subdued', 'soberly', 'reassurance', 'tame', 'daisies', 'scented', \"lady's\", 'stirring', 'coldly', 'recalling', 'slow', \"dryin'\", \"You'd\", \"Y're\", 'sprawled', 'boots', 'emphasizing', 'aspects', 'whatsoever', 'approval', 'Titus', 'sisters', 'senses', 'gently', 'impressed', 'boat', 'observed', 'laws', 'Great', 'stones', 'grand-daughter', 'heaven', 'graveyard', 'patiently', 'gentle', 'weak', 'wicked', 'crossing', 'rolling', 'roots', 'humming', 'Both', 'smaller', 'gold', 'shade', 'angel', 'tomb', 'dared', 'Miyagi', 'strain', 'ancestors', 'Asia', 'sing', 'tribal', 'hairy', 'warmed', 'South', 'occasions', 'taller', 'lucky', 'melody', 'alien', 'commission', 'granted', 'preserve', 'health', 'rule', \"Officers'\", 'hamburger', \"Navy's\", 'Coast', 'Seventh', 'liberty', 'Sooner', 'Long', 'desperately', 'Bar', 'tinkling', 'trifle', 'practice', 'tweed', 'unmarried', 'slender', 'license', 'therefore', 'quaint', 'heels', 'furiously', 'jerked', 'Uh', 'hint', 'Orient', 'beauties', 'states', 'similarly', 'Four', 'murdering', 'groomed', 'exceptionally', 'Seeing', 'pride', 'learning', 'perfectly', 'singing', 'incident', 'Back', 'uncle', 'literally', 'translated', 'strangers', \"captain's\", 'discipline', 'riot', 'Subic', 'Walt', 'expert', 'Doolittle', 'awful', 'practical', 'jokes', 'sobering', 'cockroaches', 'security', 'uniform', 'confines', 'naval', 'establishment', 'suited', 'dug', 'Naval', 'sonofabitch', 'exclaimed', 'affection', 'dock', 'dusk', 'exercise', 'golf', 'clubs', 'Sierras', 'Toodle', 'Williams', 'Imagine', 'pleasant', 'territory', 'dinners', 'timid', 'lobby', 'disappointed', 'Alaska', 'Eskimos', 'highball', 'apparently', 'satisfied', 'reaction', 'winking', 'outsiders', 'remarks', 'curiously', 'travel', 'salesmen', 'prayer', 'personnel', 'cosmetics', 'appointment', 'calm', 'dining', 'mail', 'bulletins', 'issued', 'asking', 'glisten', 'River', 'adventurous', 'flowerpot', 'spinning', 'seventeen', 'surprisingly', 'gamblers', 'caring', 'gazed', 'mused', 'cleaned', 'entrance', 'support', 'period', 'blackjack', 'parts', 'Nice', 'pushing', 'neglected', 'glorified', 'den', 'waned', 'pistol', 'cash', 'withdrew', 'customers', 'points', 'glamorous', 'frightened', 'oil', 'Tahoe', 'brings', 'properly', 'During', 'opportunity', 'communicate', 'glances', 'dangers', 'chilling', 'heavers', 'Green', 'information', 'sprang', 'revealed', 'topgallant', 'crew', 'struck', 'bellowed', 'response', 'shackled', 'Adrien', 'Deslonde', \"Alexander's\", 'intention', 'confusion', 'terror', 'potential', 'ominous', 'tones', 'Midshipman', 'Tillotson', 'brandishing', 'weapon', 'flush', 'boast', 'investigation', 'questioning', 'court', 'loose', 'flag', 'self-confident', 'appointed', 'murdered', 'harbor', 'instinct', 'Anything', 'midshipmen', 'responsibility', 'uncertain', 'trust', 'wept', 'dedicated', 'seek', 'sobbed', 'fame', 'career', 'safely', 'boatswain', 'squinting', 'shifted', 'inevitable', 'prisoner', 'shackles', 'action', 'fantastic', 'apprentices', 'awaited', 'fur', 'erect', 'cloth', 'dingy', 'heel', 'plodding', 'schools', 'occupied', 'ajar', 'steep', 'baggy', 'skullcap', 'greeted', 'flicked', 'preceded', 'corridor', 'classroom', 'benches', 'shrilled', 'interpretation', 'Books', 'chanted', 'Hebrew', 'tune', 'Each', 'rocked', 'prayed', 'portion', 'rapping', 'incessantly', 'freckles', 'barely', 'accompanied', 'pitched', 'shuddered', 'coin', 'handed', 'thanked', 'Does', 'appear', 'resented', 'faced', 'clicked', 'dangerous', 'slyly', 'wherever', 'helping', 'countries', 'farms', 'clasped', 'clamped', 'relatives', 'retrieve', 'imprisoned', 'hanged', 'energy', 'bright-eyed', 'Anyway', 'upset', 'waking', 'poling', 'fork', 'fellowship', 'pine', 'Patrol', 'gripping', 'blow', 'underwear', 'pregnant', 'dizzy', 'normal', 'daddy', 'raining', 'spectator', 'eggs', 'sheds', 'unlocked', 'discover', 'believed', 'resting', 'works', 'sees', 'plastered', 'afford', 'fill', 'Christmas', 'corn', 'factories', 'uniforms', 'bands', 'symphony', 'sophisticated', 'concert', 'virtuoso', 'nodding', 'cheeks', 'drought', 'August', 'clerk', 'thirty-four', 'conditions', 'Herman', 'connections', 'spot', 'drawings', 'jackets', 'furnished', 'daring', 'cops', 'leather', 'accurate', 'editor', 'including', 'ages', 'transparent', 'furniture', 'costume', 'inner', 'vision', 'reminded', 'bottoms', 'bottles', 'exciting', 'pants', 'shared', 'cheap', 'pressure', 'awoke', 'income', \"Monmouth's\", 'ring', 'richness', 'rent', 'forty-four', 'speech', 'included', 'series', 'Jackson', 'atmosphere', 'vast', 'artists', 'inward', 'title', 'fascinated', 'intelligent', 'replied', 'Manhattan', 'Art', 'scholarship', 'sixty', 'impressionist', 'stimulation', 'farther', 'picking', 'greater', 'bunch', 'cafe', 'clock', 'lonely', 'mostly', 'worship', 'registration', 'Ida', 'accusing', \"Via's\", 'Walter', 'tear', 'killing', 'enjoying', 'chicken', 'frozen', 'wiser', 'foggy', 'crept', 'discussed', 'annual', 'accounts', 'testimony', 'Thaxter', 'driveway', 'nowhere', 'screamed', 'search', 'closets', 'telephoning', 'recalled', 'alarm', 'conversations', 'cliff', 'romantic', 'enormous', 'account', 'vigilance', 'pity', 'void', 'self-pity', 'triumphantly', 'haunted', 'coarse', 'insulting', 'harsh', 'altogether', 'free', 'Ellen', 'minister', 'glistening', 'doll', 'stranger', 'burial', 'parlor', 'surface', 'roses', 'knelt', 'Heaven', 'grief', 'Together', 'bachelor', 'downtown', 'needs', 'purse', 'lipstick', 'solemnly', 'Alberto', 'lies', 'using', 'falling', 'exasperation', 'respect', 'shoe', 'Italian', 'outfit', 'ruins', 'amusing', 'scandal', 'bells', 'ringing', 'Steps', \"Peter's\", 'shaking', 'Bible', 'Chapel', 'Could', 'honestly', 'imagination', 'Being', 'strict', 'Ferraro', 'lap', 'utterly', 'bobbing', 'Canada', 'bush', \"Sam's\", 'boxcars', 'Holy', 'assistance', 'doctors', 'saint', 'cardinals', 'Signor', 'Raymond', \"Carla's\", 'dreamy', 'Dookiyoon', 'Shades', 'stiff', 'facing', 'contempt', 'bleeding', 'Going', 'queer', 'lodges', 'Standing', 'indignant', 'begging', 'hunger', 'bearing', 'indignation', 'shower', 'sting', 'pack', 'teach', 'Keep', 'worrisome', 'smelling', 'whisky', 'TuHulHulZote', 'concerns', 'twisted', 'moon', 'crack', 'settling', 'motionless', 'bark', 'rough', 'edges', 'lash', 'despair', 'settle', 'Turning', 'prophesied', 'Sarpsis', 'wings', 'file', 'hurtling', 'maleness', 'parade', 'nation', \"men's\", 'hearts', 'mounted', 'disheveled', 'hoofs', 'roll', 'monster', 'plunging', 'terrifying', 'pulse', 'clutch', 'downward', 'leaves', 'volume', 'miracle', 'confused', 'Hello', 'obvious', 'wire', 'pretense', 'acute', 'Bentley', 'robe', 'Monday', 'vacuum', 'cleaner', 'expanding', 'shivered', 'wound', 'Alex', 'debt', 'Too', 'adhesive', 'tape', 'however', 'Hall', 'hallway', 'wreck', 'reply', 'gazing', 'brow', 'considerate', 'clearly', \"It'll\", 'disappeared', 'footsteps', 'Pietro', 'forced', 'affectionate', 'respected', 'theater', 'Someone', 'jury', 'struggle', 'tactful', 'facts', 'routine', 'strip', 'constructed', 'secretary', 'train', 'peculiar', 'Websterville', 'anxiety', 'finds', 'Remember', 'keys', 'horizon', 'unhappily', 'startled', 'selfish', 'bundle', 'instant', 'warning', \"Susan's\", 'entire', 'dust', 'breeze', 'darkened', 'radiant', 'nests', 'baffled', \"people's\", 'airy', \"Greg's\", 'faintly', 'lavender', 'buried', 'pause', 'wallpaper', 'hesitated', 'Christmastime', 'speechless', 'teddy', 'delivered', 'jammed', \"Quint's\", 'cottages', 'V-shaped', 'inlet', 'seaweed', 'whiskers', 'blades', 'controlling', 'Fearless', 'piece', 'Fortman', 'Stuck-up', 'loud', 'Gord', 'sweat', 'Aw', 'hole', 'shoulder', 'choppy', 'peppermints', 'Ever', 'Gordon', 'tail', 'soup', 'shaken', 'mackerel', 'yodeling', 'Same', 'King', 'Which', 'mold', 'fought', 'smothered', 'occasion', 'tonsil', 'giggles', 'swam', 'plastic', \"he'll\", 'Fifteen', 'Yeah', 'rope', 'lung', 'caused', 'Jaguar', 'thoroughly', 'chapter', 'community', \"Wasn't\", 'Mountains', 'Dartmouth', 'engagement', 'events', 'jolt', 'Cleveland', 'wishful', 'absolutely', 'shallow', 'frivolous', 'scatterbrained', 'magnificent', 'example', 'correct', 'Others', 'trace', \"Edythe's\", 'glaring', 'graham', 'crackers', 'problem', 'match', \"Bobbie's\", 'decent', 'Smith', '&', 'salesman', 'branch', 'contacts', 'Murkland', 'harder', 'feels', 'opportunities', 'surprise', 'degree', 'sweetly', 'falls', 'staying', 'refuse', 'Pack', 'davenport', 'instantly', 'strode', 'nails', 'state', 'bubble', 'miserably', 'coy', 'unbelievable', 'tide', \"They'd\", 'spoiled', 'bum', 'application', 'logic', 'thousandth', 'complain', 'Hodges', \"mustn't\", \"Gladdy's\", 'credit', 'Michelson', 'promising', 'chatter', 'rounds', 'sights', 'chart', 'diagnosis', 'familiar', 'network', 'violent', 'patient', 'control', 'examination', 'penetrating', 'smear', 'Doctor', 'permission', 'shiver', 'amazement', 'cancer', 'echo', 'outta', 'nitrogen', 'contented', 'mouthpiece', 'aloud', 'vacation', 'Valery', 'army', 'Hun', 'Jour', 'et', 'Nuit', 'Montmartre', 'cheese', 'drained', 'Tropic', 'breathing', 'topcoat', 'umbrella', 'hustler', 'Hardly', 'lied', 'wager', 'Monsieur', 'sadly', 'cynicism', 'rotten', 'thirty-five', 'forty', 'sensed', 'Bon', 'jour', 'hips', 'brushed', 'innocently', 'brush', 'Stephen', 'cups', 'Louvre', 'Express', 'leaning', 'Today', 'Colosseum', 'prescribed', 'noisy', 'frightening', 'Shoals', 'smartly', 'splendid', 'staircase', 'display', 'azalea', 'workmen', 'azaleas', 'blooms', 'purple', 'learned', 'warming', 'cigarette', 'confidence', 'drawer', 'replaced', \"Elec's\", 'grandchildren', \"Emma's\", 'contorted', 'solemn', \"George's\", 'persuade', 'possibility', 'arrangements', 'separate', 'specialized', 'alligator', 'Finally', 'scurried', 'supremely', 'exist', 'Elsie', 'motion', 'Westfield', 'Dolan', 'Young', \"Christians'\", 'faith', \"guy's\", 'willing', 'kitten', 'honorable', 'hopelessly', 'surrendered', 'motel', 'desire', 'awareness', 'Mmm', 'elbow', 'maturity', 'Boston', 'blond', 'separated', 'Need', 'invisible', 'appeal', 'Unit', 'Number', 'lightly', 'victory', 'Nassau', 'Bobbsey', 'Twins', 'variety', 'presence', 'onion', 'wrapping', 'feminine', 'judgment', 'following', 'chic', 'subconsciously', 'collection', 'Larkspur', 'Clearly', 'ninety-six', 'browny-haired', 'sprinkling', 'sympathetic', 'tan', 'Third', \"Deegan's\", 'bastard', 'spectators', 'fielder', 'ballplayer', 'built', 'mound', 'arose', \"Phil's\", 'sweatshirt', 'jaw', 'locker', 'banged', 'professional', 'leagues', 'abruptly', 'shaky', 'product', 'numerous', 'Through', 'seldom', 'studies', 'acquired', 'freighter', 'eaten', 'pretentious', 'throughout', 'Ceecee', 'statue', 'paced', 'sensation', 'hillside', 'demon', \"Fudo's\", 'refusing', 'plowing', 'stubble', 'area', 'lath', 'panic', 'tapered', 'frighten', 'trailed', 'delicately', 'diamonds', 'gracefully', 'recently', 'solid', 'clods', 'pirouette', 'brutal', 'eagerly', 'scholastic', 'record', \"son's\", 'organization', 'Ivy', 'grades', 'elected', 'slammed', 'forum', \"David's\", 'Products', 'Half', 'colleges', 'Hear', 'surprising', 'bass', 'Crazy', 'Horse', 'Colts', 'generally', 'garments', 'resistance', 'stubborn', 'contest', 'lace', 'chain', 'forgot', 'Bryan', 'corporation', 'contracts', \"Willis'\", 'partner', 'power', 'conversion', 'allied', 'aircraft', 'jet', 'automobile', 'promotion', \"Hamrick's\", 'Motors', 'gantlet', 'tower', 'Mass', 'repetitive', 'monotonous', 'unimportant', 'bees', 'tomato', 'paste', 'sour', 'aluminum', 'trays', 'fly-dotted', 'cheesecloth', 'surging', 'bodies', 'dived', 'blackness', 'amber', 'bay', 'Filippo', 'Rossi', 'Signore', 'elders', 'Youth', 'fur-piece', 'wiggled', 'satin-covered', 'buttocks', 'clutched', 'straw', 'strutted', 'streetcars', 'bigger', 'fancy', 'airs', 'ours', 'speeches', 'Dante', 'actresses', 'Henh', 'Calloused', 'polished', 'excitedly', 'puckered', 'chins', 'hairs', 'sprouted', 'tweezed', 'Mauve-colored', 'mouths', 'Puttana', 'fleshy', 'suppleness', 'seams', 'shooing', 'fleas', 'hopped', 'sundials', 'variegated', 'expression', 'flown', 'withered', 'streetcar', 'contemptuous', 'purpose', 'opposition', 'outskirts', 'Philadelphia', 'Bari', 'Chieti', 'Ash', 'Road', 'elevated', 'trains', \"city's\", 'half-hour', 'creek', 'Schuylkill', 'aloneness', 'framed', 'ginkgo', 'lined', 'two-story', 'slashed', 'manure-scented', 'lawns', 'wicker', 'swings', 'rusted', 'hinges', 'stable-garage', 'rot', 'evenings', 'Sundays', 'reeked', 'dregs', 'squatted', 'sidewalk', 'grating', \"Bartoli's\", 'second-story', 'showroom', 'angels', 'surveyed', 'sameness', 'perched', 'slant', 'paved', 'alleyways', 'tunneled', 'core', 'intimacy', 'backyards', 'fences', 'blended', 'courtyards', 'vines', 'Waiting', 'pact', 'heritage', 'ended', 'white-columned', 'eight-thirty', 'local', 'pound', 'gloved', 'vest', 'gloves', 'unconcerned', 'gilded', 'lithe', 'straddled', 'railing', 'loosely', 'creaking', 'pales', 'artfully', 'tapering', 'tips', 'glowed', 'sinewy', 'swirled', 'childlike', 'softness', 'fragile', 'harshness', 'belied', 'lyric', 'contours', 'downcast', 'possessed', 'clouded', 'screeching', 'rail', 'daydreaming', 'wetness', 'claret', 'feasting', 'salt', 'sallow', 'time-cast', 'encrusted', 'marbleized', 'unfalteringly', 'git', 'Soothing', 'whiskered', 'expectantly', 'undulated', 'gradually', 'covering', 'squirted', 'savored', 'earthy', 'operated', 'skilled', 'unity', 'bagpipe', 'pressing', 'pulling', 'delighting', 'evasive', 'cloud', 'Its', 'fluttering', 'soutane', 'sensing', 'portentous', \"dog's\", 'Time', 'meantime', 'stained', 'ocher', \"Pompeii's\", 'edged', 'clapping', 'ecstatic', 'released', 'haunches', 'ticks', 'biggest', 'Niobe', 'neatest', 'Concetta', 'laced', 'Romeo', 'idiot', \"Romeo's\", 'grasp', 'impetuous', 'rotundity', 'throbbed', 'fatigue', 'earnest', 'quench', 'outdistanced', 'recovery', 'highly', 'fuzz', 'flaxen', 'rages', 'digesting', 'peaches', 'cream', 'disposition', 'amounts', 'stated', 'lifting', 'coaxed', 'rackety', 'washer', 'daily', 'chores', 'participating', 'Worry', 'produce', 'salary', 'Clifton', 'preferred', 'bankruptcy', \"family's\", 'vitamins', 'pored', 'squeeze', 'pennies', 'consulted', \"Woman's\", 'Exchange', 'goods', 'starched', 'nineteen', 'nuzzled', 'ironed', 'wrapper', \"Best's\", 'Liliputian', 'Bazaar', 'sensible', 'pencil', 'jotting', 'mothers', 'cribs', 'playroom', 'entail', 'salads', 'beans', 'gypsy', 'fortunes', 'clearing', 'chattering', 'contests', 'November', 'miseries', 'countless', 'temperature', 'zero', 'tangible', 'pall', 'Mile', 'High', 'locking', 'harshened', 'outlines', 'realism', 'resembling', 'habitual', 'sprawling', 'bumptious', 'open-handed', 'basking', 'stored', 'riches', 'jobless', 'employment', 'parks', 'exclusive', 'created', 'depressions', 'markets', 'congested', 'populations', 'centralization', 'industries', 'discriminatory', 'freight', 'popularly', 'creating', 'penetrated', 'spending', 'remedies', 'millions', 'urgent', 'Abernathys', 'stretch', 'mortgage', 'taxes', 'overshoes', 'leaks', 'emergencies', 'termed', 'major', 'disaster', 'maw', 'appeased', 'hurling', 'tons', 'Cold', 'innumerable', 'sprung', 'frames', 'attic', 'shingles', 'capacity', 'fireplaces', 'electric', 'gaiety', 'infected', 'stormbound', 'shipwrecked', 'circumstance', 'anxieties', 'cling', 'survive', 'pipes', 'ankles', 'swabbed', 'bathrooms', 'groaning', 'polar', 'regions', 'mining', 'joys', 'emergency', 'wallow', 'willed', \"Hope's\", 'tipped', 'overturning', 'Thrifty', 'Unusual', 'tenant', 'simplify', 'California', 'jilted', \"sun's\", \"Brace's\", 'sneaky', 'consorting', 'tenants', 'understands', 'Labans', 'valiant', 'pitiable', 'Returning', 'log-house', 'Jonathan', 'stormy', 'Him', 'Put', 'asunder', 'Absolution', 'telegraph', 'message', 'sealed', 'pigeonhole', 'resist', 'deception', 'peculiarly', 'confess', 'lacerate', 'Reaching', 'relic', 'pioneer', 'pyre', 'curl', 'sealing', 'wax', 'melting', 'bubbling', 'feathery', 'beloved', 'gathers', 'fruit', 'subtle', 'genius', 'Swinburne', 'gifted', 'satisfactions', 'Beyond', 'greening', 'Grand', \"parents'\", 'grandsons', 'thimble', 'Brace', 'easygoing', 'blacksmith', 'preacher', 'Howdy', 'careless', 'Oscar', 'P.GA', \"C'un\", 'Major', 'plain-out', 'carte', 'blanche', 'weakness', 'devotion', \"Robards'\", 'Queen', 'mares', 'geldings', 'what-nots', 'reverent', \"Racin'\", 'revolting', 'pearl', 'stamp', 'imperiously', 'stall', 'invariably', 'Nerves', 'nip', 'Stand', 'oneasy', 'Quit', \"sor'l\", 'lip', 'January', \"musn't\", 'annoy', 'Listening', 'predictions', 'procession', 'foals', 'symbolized', 'unpleasant', 'treated', 'acknowledge', 'condition', 'disgusting', 'Human', 'birth', 'novelty', 'midwife', \"Jenny's\", 'former', 'admirer', 'riding', 'quarts', 'liniment', 'fussing', 'bran', 'mash', 'charlotte', 'russe', 'tracking', 'manure', 'jiffy', \"trippin'\", \"ever'\", 'Rhyme', 'Arcilla', 'Flotilla', 'Edmonia', 'Jennifer', 'Kezziah', \"this'll\", 'boy-name', \"Mare's\", 'Handsomest', 'colt', 'Kentucky', 'Strong', 'Royal', 'Jesus', 'distressed', 'peach', 'Home', 'trimmed', 'callers', 'Shawnee', 'Rakestraw', 'criticisms', 'Heavenly', 'Rest', 'boarding', 'inclement', 'Spa', 'snippy', 'namesake', 'teething', 'cordial', \"Tillie's\", \"Nick's\", 'posts', 'pups', 'to-do', 'dollies', 'finest', 'spa', 'entertain', 'nigs', 'invite', \"ever'body\", 'Galt', 'House', 'Jockey', 'affair', 'dang', 'oystchers', \"bar'l\", \"oystchers'll\", 'perk', 'downright', \"Roy's\", 'buggy', 'Eph', 'Showers', 'preach', 'mountain', 'politely', 'gateway', \"Them's\", 'purtiest', 'babes', \"comin'\", 'forgiven', 'absolution', 'testament', 'proof', 'godless', 'wishing', 'garnet', 'counsel', 'inevitably', 'unfavorable', 'frequently', 'circumspect', \"Hetty's\", 'flair', 'drama', 'summers', 'Maneret', 'excellently', 'austere', 'lessons', 'Polish', 'nobleman', 'Craddock', 'supervising', 'wand', 'everlasting', 'teas', 'fetes', 'jocular', 'possessive', 'planets', 'revolved', 'corruption', 'interruption', 'assume', 'spared', \"Charles'\", 'vulnerable', 'argue', 'morality', 'consisted', 'finding', 'oppressive', 'Impossible', 'virtue', 'colloquy', 'sill', 'artificial', 'blossom', 'leaf', 'pilgrim', 'avoided', 'graveyards', 'remembering', 'lacy', 'enchanting', 'wildness', 'Leaning', 'tangle', 'rosebush', 'honeysuckle', 'bloom', 'spray', 'thorns', 'dewdrops', 'buds', 'Valentine', 'balanced', 'nondescriptly', 'artless', 'decidedly', 'une', 'femme', \"d'un\", 'unbelievably', 'protective', 'bun', 'jug', 'equivalent', 'wilderness', 'yielded', 'patchwork', 'william', 'bedstraw', 'grasses', 'haranguing', 'poodle', 'gleefully', 'supplicating', 'prayerful', 'forepaws', 'Rummaging', 'comply', 'slumbered', 'shuttered', 'shops', 'inhabitants', 'eying', 'frankest', 'curiosity', 'bowed', 'princess-in-a-carriage', 'acknowledgments', 'cautious', 'reflective', 'sap', 'plainly', 'cupped', 'repetition', 'announcing', 'nettled', 'unenthusiastic', \"Summer's\", 'scowled', 'vague', 'hospitality', \"They'll\", \"takin'\", 'pleasantly', \"soon's\", 'doubtfully', 'soldierly', 'ceased', 'pointer', 'finer', 'text', \"f'r\", 'happily', 'rests', 'womanly', 'Rests', 'pacifies', 'dad', 'link', 'nineties', 'Blackwells', 'silenced', 'scanty', 'deep-set', 'mariner', 'awed', 'fishing-boat', 'powerful', 'shriveled', \"well's\", 'Ran', 'lawful', 'wedded', \"leavin'\", 'cheerfully', 'Left', 'Any', 'dryly', 'knowingly', 'liking', 'connivance', 'Selma', \"Cotter's\", 'censure', 'praised', 'big-boned', 'drab-haired', 'apron', 'print', 'vaguely', \"gran'dad\", \"Y'r\", \"dam'\", 'porridge', 'Milk', 'sops', 'claws', 'clattered', 'lowly', 'wearily', 'piously', 'Beer', 'affect', 'ingenious', 'hypocrisy', 'forgave', 'stint', 'malevolence', \"Blackwell's\", 'somewheres', \"Ma'am\", 'Delia', 'cruelty', 'tyranny', 'mumbled', 'shuffling', 'mightily', 'buffeted', 'gabble', 'indoors', 'swearing', \"journey's\", 'dotted', 'bushes', 'sheep', 'blossoms', 'scant', 'clawing', 'stony', 'darted', 'rustle', 'placid', 'terrestrial', 'mindless', 'spire', 'lessened', 'headstones', 'Dorothy', 'Tredding', 'nearest', 'sea-damp', 'tracing', 'indecipherable', 'carved', 'padded', 'moss', 'parasol', 'rock-carved', 'nearby', 'scudding', 'fearing', 'ghosts', 'Prefecture', 'northeast', 'Honshu', 'traces', 'Ainu', 'Ainus', 'primitive', 'Southern', 'Apparently', 'Caucasian', 'skins', 'bearded', 'pitiful', 'subsist', 'chants', 'sadness', 'Indians', 'assimilated', 'Akita', 'prefectures', 'occasionally', 'strikingly', 'coloring', 'improved', 'tawny', 'honey-in-the-sun', 'tint', 'invaders', 'fortunate', 'exquisitely', 'willowy', 'susceptible', 'vegetable', 'diet', 'forebears', 'intriguingly', 'rebellious', 'requests', 'Bremerton', 'Lakes', 'Pensacola', 'threat', 'resign', 'Anywhere', 'assigned', 'psychopathic', 'depressingly', 'cases', 'teamed', 'chaplain', 'counselor', 'mental', 'salvage', 'firms', 'operate', 'hulks', 'warships', 'sunk', 'inshore', 'setting', 'nerve-shattering', 'blasts', 'psychiatry', 'off-duty', 'sustain', 'dinnertime', 'cocktail', 'crossroads', 'Ships', 'rotated', 'six-month', \"Fleet's\", 'port', 'maintenance', 'shore', 'ships', 'officers', 'good-looking', 'cubes', 'tipsy', 'consequently', 'tailored', 'carriers', 'aviator', 'fifty-fifty', 'overseas', 'implied', 'excused', 'intimated', 'Harro', 'girl-san', 'catchee', 'boy-furiendo', 'likee', 'beg', 'pardon', 'brand', 'nice-looking', 'open-mouthed', 'blushing', 'blush', 'gesture', 'slang', 'fly-boy', 'lasting', 'satisfaction', \"nurses'\", 'brunettes', 'Moorish', 'Balkan', 'endowed', 'blessed', 'halfway', 'humiliated', 'self-consciously', 'waitresses', 'counter', 'Yuki', 'Kobayashi', 'Bifutek-san', 'Kohi', 'Futotsu', 'whitehaired', 'doting', 'commander', 'appealing', \"Tommy's\", 'rebelliously', 'civilian', 'transient', 'increasingly', 'Nurse', 'Corps', 'congenial', 'after-duty', 'raucous', 'poorly', 'companionship', 'put-upon', 'repeated', 'Eating', 'indigestion', 'bicarbonate', 'soda', 'sulked', 'crude', 'Oyajima', 'kimono', 'faculty', 'hostess', 'gown', 'kotowaza', 'proverb', 'Tanin', 'yori', 'miuchi', 'Relatives', 'thicker', \"Doolittle's\", 'scheduled', 'dispensed', 'ordinarily', 'immature', 'tactics', 'rabble', 'rousing', 'belief', 'fun-loving', 'Guns', 'Appleby', 'perils', 'horseplay', 'stabilizing', 'influence', 'overdeveloped', 'wisecracked', 'indoctrination', 'slaughtering', 'ethyl', 'chloride', 'Armed', 'Forces', 'heights', 'eventually', 'retired', 'restriction', 'Bustard', 'deprived', 'Boats', 'McCafferty', 'restricted', 'thoughtful', 'medical', 'supplies', 'Supply', 'inspect', 'caves', 'shipmate', 'connect', 'exchange', 'Gresham', 'commissary', 'Grab', 'errand', 'salute', 'gangway', 'Petty', 'excursion', 'boating', 'hiking', 'virile', 'strenuous', 'fashionable', 'Hotel', 'frog', 'packing', \"Buzz's\", 'Wow', 'Strippers', 'scrumptious', 'all-lesbian', 'band', 'Hi', 'Willows', 'notes', 'Tough', 'Reno', 'instigator', 'victims', 'rap', 'Ahah', 'lush', 'divorcee', 'scrawny', 'pajamas', 'moons', 'semi-professionally', 'sponsoring', 'courageous', 'sparkled', 'dully', 'hardships', 'undergo', 'Smug', 'smug', 'sappy', 'twitch', 'region', 'lewd', 'Eskimo', 'sickly-tolerant', 'canvassing', 'Forebearing', 'Several', 'wandering', 'corridors', \"strangers'\", 'difficulties', 'pairs', 'hundred-and-fifty', 'Northwest', 'overexcited', 'remonstrated', 'secular', 'bibles', 'publishing', 'appliances', 'waspishly', 'appliance', 'heatedly', 'blinking', 'seller', 'Leave', 'bucking-up', 'crestfallen', 'race', 'indecisively', 'receive', 'enthusiastic', 'statement', 'overdue', 'alimony', 'Collector', 'Internal', 'Revenue', \"year's\", 'exemptions', 'Virginia', 'braced', 'crest', 'white-topped', 'Truckee', 'spangle', 'elated', 'rattling', 'roulette', 'neon', 'automatically', 'mural', 'depicted', 'settlers', 'wagons', 'animated', 'expectancy', 'splendidly', 'stern', 'Donner', 'starving', 'heavily-upholstered', 'one-arm', 'bandits', 'cunningly', 'slots', 'bested', 'mechanical', 'devices', 'depending', 'dried-up', \"blonde's\", 'stables', 'martingale', 'hit-and-miss', 'five-seventeen', 'logging', 'twelve-hour', \"roulette's\", 'Incidentally', 'famous', 'ranch', 'Bar-H', 'collect', 'rack', 'single-foot', 'fastest', 'Washoe', 'County', 'publicity', 'campaign', 'TV', 'hero', 'trained', 'Hoot', 'Gibson', 'pinto', 'photographs', 'wonderfully', 'conclusively', 'jinx', 'gambling', 'marking', 'keno', 'featured', 'attraction', 'ogled', 'shill', 'prejudice', 'shills', 'flipping', 'discreetly', 'chips', 'premium', 'announcement', 'whereby', 'feature', 'floorshow', 'A.M.', 'starring', 'Adele', 'Body', 'Brenner', 'schoolgirls', 'brides', 'orchids', 'chuck-a-luck', 'Hawaiian', 'sun-tan', 'shorter', 'fatter', 'Lake', 'Cal-Neva', 'instigating', 'guarantees', 'stickman', 'pit', 'crossroading', 'faro', 'allergic', 'pollen', 'okay', 'crap', 'pyramid', 'consecutive', 'platinum', 'bursting', 'sun-suit', 'well-fed', 'prosperous', 'propositioned', 'Stake', 'tango', 'discuss', 'merger', 'Sounds', 'Gisele', 'Scotch', 'Named', 'ballet', 'Sylphide', 'affected', 'Answer', 'Very', 'procedure', 'cold-bloodedly', 'chide', 'lapse', 'manufacturing', 'steal', 'aft', 'pretence', 'Hostile', 'flashed', \"Captain's\", 'arrests', 'E.', 'Andrews', 'oldest', 'glories', 'identified', 'mutineer', 'tap', 'Oliver', 'breathless', 'wild-eyed', 'holystones', 'mysteriously', 'customary', 'articles', 'souvenirs', 'African', 'battle-ax', 'sharpened', 'overheard', 'disappearance', 'ladder', 'aimless', 'milling', 'well-trained', 'well-organized', 'horror', 'orders', 'alert', 'questioningly', 'instruction', 'hastened', 'weather-royal', 'consultation', 'mistaken', 'aiding', 'obey', 'bloodshed', 'strategy', 'reasoned', 'defeated', 'unwillingness', 'sanction', 'originated', 'stalked', 'openly', 'morose', 'muster', 'expressing', 'displeasure', 'communicating', 'hiding', 'insolently', 'disobeyed', 'Seaman', 'attacked', 'snarling', 'McKee', 'brig', 'loyal', 'villainous', 'unaffected', 'crime', 'mutiny', 'fears', 'requires', 'omniscient', 'select', 'rely', 'staunch', 'modest', 'ventured', 'guard', 'increased', 'inquiry', 'punished', 'perform', 'workable', 'extravagant', 'agree', 'thoughtfully', 'conspiracy', 'ferocious', 'anarchy', 'Implements', 'available', 'hasty', 'combat', 'ourselves', 'sentinel', 'brothers', 'labor', 'accordance', 'implacable', 'protection', 'honor', 'yearned', 'passionately', 'strive', 'gates', 'eighteen-year-old', 'sailed', 'shed', 'desired', \"egotist's\", 'precious', 'satisfy', 'egotist', 'served', 'Dear', 'dried', 'assurance', 'Stern-faced', 'inspected', 'satisfying', 'averted', 'glimpsed', 'mild-mannered', 'respectful', 'seeming', 'lethargy', 'gaze', 'calmness', 'detachment', 'unawareness', 'implicit', 'naive', 'thick-skulled', 'sudden', 'clarity', 'kinship', 'stupidity', 'quarreling', 'handcuffs', 'ankle', 'follows', 'misfortune', 'towering', 'symbol', 'authority', 'tragic', 'lad', 'forged', \"other's\", 'undoing', 'deny', 'hazel', 'accept', 'gain', 'defending', 'contemplation', 'logical', 'untruth', 'interpreted', 'prior', 'significance', \"Cromwell's\", 'explanations', 'Rogers', 'geometry', 'wardroom', 'element', 'absurdity', 'percentage', 'steered', 'Torah', 'Bits', 'trash', 'roadway', 'warmish', 'foul', 'strides', 'double-breasted', 'material', 'buckles', 'brim', 'crown', 'trim', 'pinkish-white', 'conscious', 'long-sleeved', 'stride', 'pivoting', 'twisting', 'discolored', 'boarded', 'synagogues', 'shabby', 'clasping', 'unclasping', 'paunch', 'bare-armed', 'dripped', 'poised', 'surly', 'half-closed', 'Rapping', 'translation', 'Moses', 'previously', 'grandfather', 'great-grandfather', 'upturned', 'cropped', 'ringlets', 'framing', 'yellowed', 'prayerbooks', 'penalty', 'distraction', 'guttural', 'ghetto', 'curls', 'traveled', 'partially', 'texts', 'memorized', 'singsonged', 'off-key', 'baritone', 'spurred', 'tapping', 'defined', 'rhythm', 'backed', 'clucked', 'high-pitched', 'devote', 'Except', 'Shabbat', 'praying', 'numb', 'prickly', 'asks', 'fingered', 'pruta', 'Sabras', 'roads', 'Messiah', 'convictions', 'immortal', 'heroic', 'twinkling', 'respectfully', 'kibbutzim', 'Aliah', 'immigrants', 'Ready', \"Me'a\", \"She'arim\", 'Jewish', 'anyplace', 'gaining', 'rebuffed', 'slowed', 'cobblestones', 'pursed', 'Trouble', 'middle-aged', 'strut', 'numerals', 'branded', 'concentration', 'Often', 'despondent', 'wandered', 'official', 'Mandate', 'arrested', 'direct', 'complained', 'intend', 'proposal', 'straighten', 'beaten', 'chariot', 'south', 'Forked', 'Deer', 'braving', 'wastes', 'dumped', 'barge', 'Ethiopians', 'tooling', 'sweeping', 'expansiveness', 'courts', 'cabins', 'bulb', 'unfrosted', 'streetlight', 'swooping', 'bugs', 'bouts', 'hopscotch', 'moths', 'pinging', 'cruising', 'insect', 'Highway', 'feelers', 'sagged', 'overhearing', 'curtain', 'rehearsal', 'Mattie', 'microphone', 'Toonker', 'Burkette', 'yanking', 'leak', 'burn', 'parachute', 'Starkey', 'Poe', 'draining', 'punctured', 'muddy', 'pumps', 'squat', 'son-of-a-bitch', 'hating', 'raped', 'tool', 'rape', 'nestled', 'breast', 'tramp', 'skull', 'drives', 'delivers', 'berry', 'crates', 'owns', 'wells', 'mines', 'mild-voiced', 'little-town', 'big-town', 'level', 'Houdini', 'mile', 'chump', 'too-shiny', 'radio', 'dawns', 'seat', \"Shafer's\", 'substitute', 'Louis', 'Damn', 'acres', 'whitewashed', 'bordering', 'grounds', 'envisioned', 'Homes', 'federal', 'highway', 'peaceful', 'bandstand', 'flash', 'instruments', 'spellbound', 'piano', 'rehearsing', 'governor', 'swelled', 'goose', 'bumps', 'rippled', 'Tennessee', 'farmer', 'rained', \"rat's\", 'ass', 'furrowed', 'powdery', 'droughts', 'predicting', 'festival', 'Factory-to-You', 'maids', 'youngest', 'unventilated', 'allow', 'malingering', 'primping', 'sincere', 'scrubbing', 'boxed-in', 'public', 'count', 'rapped', 'jerk', 'whichever-the-hell', 'six-thirty', 'Saturdays', 'chairs', 'fans', 'porter', 'janitor', 'Among', 'handled', 'commissions', 'uptown', 'lettering', 'specialty', 'complicated', 'pen-and-ink', 'medieval', 'hundreds', 'crammed', 'eight-by-ten', \"jeweler's\", 'precise', 'spots', 'literary', 'artistic', 'journals', 'Yorker', 'Esquire', 'paperback', 'reprints', 'huddling', 'corners', 'sleeves', 'revolver', 'Brothers', 'Karamazov', 'illustration', 'Magpie', 'Press', 'historical', 'novel', 'Edward', '3', 'fifteenth-century', 'ferreted', 'materials', 'shields', 'linden', 'reflections', 'sketch', 'Rufus', 'static', 'reproduce', 'readers', 'researches', 'television', 'movies', 'knights', 'flowing', 'haircuts', 'Fauntleroy', 'villains', 'beards', 'Byron', 'sword', 'stills', 'movie', 'clam', 'authenticity', 'painters', 'scholarships', 'Joyce', 'style', 'adults', 'freshness', 'perception', 'self-consciousness', 'twist', 'forms', 'leap', 'smack', 'nearsighted', 'prevent', 'uncanny', 'absent-minded', 'sleepwalker', \"Christ's\", 'Prussian', 'discovery', 'gravy', 'expenses', 'lodgings', 'Attending', \"Askington's\", 'ambition', 'reputation', 'illustrator', 'Peter', 'goal', 'cashmere', 'buttons', 'Viyella', 'necktie', 'bolo', 'jade', 'texture', 'clothing', 'decorations', \"Brush-off's\", 'sparkle', 'erudite', 'illustrators', 'cover', 'magazines', 'Modern', 'Artists', 'photographed', 'Sixties', 'Velasquez', 'royalty', 'Spain', 'bookshelves', 'Modigliani', 'portrait', 'Pollock', 'Miro', 'background', 'inscription', 'Martian', 'fireplace', 'tiles', 'Picasso', 'restaurants', 'illusion', 'wealthy', 'craft', 'absorb', 'masters', 'Durer', 'Bellini', 'Mantegna', 'Painting', 'interdependent', 'varied', 'multitudinous', 'fragment', 'mosaic', 'performer', 'philharmonic', 'organize', 'gigantic', 'abstract', 'expressionism', 'photorealism', 'outward', 'Eye', 'anatomy', 'teaches', 'five-hundred-dollar', 'prize', 'bums', 'Hudson', 'enameling', 'Hajime', 'Iijima', 'Osric', 'million', 'retrospective', 'contemporary', \"Cezanne's\", 'Still', 'canvases', \"Thoreau's\", 'hangouts', 'magical', 'Contact', 'stimulating', 'succeed', 'isolating', 'occupation', 'Middle', 'Ages', 'Renaissance', 'nineteenth', 'century', 'artisan', 'craftsmanship', 'goldsmith', 'carver', 'society', 'portraying', 'wars', 'impulses', 'propagandist', 'satirist', 'lover', 'philosopher', 'scientist', 'illustrating', 'tooth-paste', 'ads', 'salacious', 'incidents', 'trivial', 'novels', 'fluff', 'navel', 'contemplated', 'purity', 'amateur', 'dervishes', 'apprenticeship', 'upshot', \"Pendleton's\", 'Thursday', 'awkward', 'punch', 'drawing', 'finish', 'Brush-off', 'benefit', 'mailman', 'Stimulating', 'rewarding', 'Partly', 'anticipated', 'prepared', 'classic', 'cigars', 'quitting', 'compartment', 'readily', 'frowning', 'mutually', 'stir', 'courtesy', 'fraud', \"Walter's\", 'assail', 'temporary', 'narrows', 'half-murmured', 'statements', 'stress', 'indulge', 'sequence', 'noble', 'sponge', 'dessert', 'roasted', 'parboiled', 'vegetables', 'icebox', 'appointments', 'Thaxters', 'recovering', 'movie-to-be', 'London', 'tranquil', 'tolerance', 'patch', 'grooved', 'accustomed', 'dense', 'mists', 'sun-warmed', 'palisades', 'twinkle', 'woods', 'shopping', 'parish', 'bazaar', 'Prisoners', 'accused', 'accident', 'unsee', \"car's\", 'turnaround', 'Engisch', 'exclaiming', 'rushes', 'gasps', 'portfolio', 'lingerie', 'mink', \"Salter's\", 'servant', 'sandwiches', 'Sitting', 'hour-long', 'Constance', 'plea', 'Hanging', 'patience', 'stab', 'conclusion', 'winds', 'gleaming', 'moontrack', 'poetic', 'misstep', 'fisherman', 'triumphant', 'hysterical', 'Sonny', 'shuddering', 'pitied', 'underneath', 'unquenched', 'enduring', 'ghastly', 'appalled', 'exclamation', 'estranged', 'hangs', 'separation', 'prevented', 'justify', 'scapegoat', 'borne', 'avoidance', 'rankles', 'coolly', 'hurts', 'Mathias', 'persisted', 'explaining', 'advised', \"nobody's\", 'overplayed', 'appraisal', \"Mathias'\", 'towns', 'dispossessed', 'makeshifts', 'arid', 'discarded', 'risen', 'fullest', 'height', 'transcending', 'murky', 'self', 'ignorant', 'self-examination', 'conclusions', 'realizing', 'praise', 'deliverance', 'pettiness', 'greed', 'Methodist', 'eyeglasses', 'drumming', 'accompaniment', 'relentless', 'postponed', 'cremate', 'leaped', 'trestles', 'sheaf', 'whispering', 'parking', 'hearse', 'Jersey', 'chapel-like', 'auditorium', 'discreet', 'symbols', 'faiths', 'impelled', 'kneel', 'Bach', 'healing', 'Lancaster', 'Arms', 'eyelid', 'convivial', 'Pausing', 'Me', 'Umm', 'uhhu', 'Kleenex', 'wiped', 'flatter', 'vanity', 'Caneli', 'stories', 'cleansing', 'orderly', 'hide', 'annoyance', 'sloppy', 'scrub', 'vent', 'dwelt', 'indignities', 'view', 'wears', 'thin-soled', 'dark-gray', 'slacks', 'fawn-colored', 'encountered', 'tweedy', 'Englishman', 'delighted', 'Changing', 'dark-blue', 'remind', 'Parioli', 'loafed', 'Roman', 'Veneto', 'Farnese', 'Gardens', 'Farneses', 'Trastevere', 'piazza', 'Santa', 'Maria', 'eloquent', 'scolding', 'obelisk', 'clung', \"Aren't\", 'drowsily', 'certainty', 'temperament', 'achieved', 'tranquility', 'composure', 'accepting', 'Testament', 'Sistine', 'folklore', 'prophets', 'Catholic', 'Protestant', 'dogmatic', 'atheists', 'Communist', 'wearied', 'ideologies', 'enlargement', 'wearying', 'pondering', 'hurrying', 'joking', 'stair', 'behave', 'softly', 'Ciao', 'gleamed', 'gestures', 'courteous', \"'ello\", 'biscuits', 'Acting', 'interpreter', 'impersonal', 'Watching', 'bounced', 'encouragingly', 'mill', 'seasonal', 'unemployment', 'migrated', 'railway', 'countrymen', 'Regretfully', 'adopted', 'sparkling', 'intellectual', 'mystical', 'Portugal', 'confirmation', 'serenity', 'unaware', 'homely', 'enchantment', 'translate', 'rulers', 'Nodding', 'approvingly', 'confidentially', 'discontent', 'splendor', 'intellect', 'regime', 'Jobs', 'Italians', 'Devout', 'Brooklyn', 'Malta', 'Ireland', 'glowing', 'softening', 'purify', 'caressing', 'flageolet', 'whisper', 'emerge', 'unhesitant', 'encounter', 'disdain', 'outraged', 'proceeded', 'destination', 'bounce', 'press', 'feared', 'presences', 'metabolism', 'weird', 'invested', 'eyeballs', 'thumbs', 'fled', 'minded', 'Elsewhere', 'clusters', 'half-drunk', 'mild', 'commotion', 'hushed', 'imperious', 'denying', 'dread', 'suffer', 'insult', \"girl's\", 'scorn', 'overloud', 'shriek', 'Telling', 'unsheathing', 'curing', 'hides', 'wickedness', 'cunning', 'Speaking', 'loathing', \"helsq'iyokom\", 'murmuring', 'knot', 'threats', 'mingling', 'hurled', 'embodiment', 'defiance', 'howling', 'bullhide', 'tripping', 'seized', 'flog', 'Drive', 'jerking', 'hovel', 'double-married', 'parades', 'confront', 'leads', 'fury', 'stirred', 'mumbling', 'stupor', 'wolves', 'nakedness', 'stale', 'odor', 'Lie', 'curses', 'gnarled', 'talons', 'rudely', 'shoving', 'spruce', 'shortening', 'arc', 'narrowed', 'snarled', 'cracked', 'unchanged', 'delineaments', 'begotten', 'forbidden', 'tracings', 'tree', 'refracted', 'crazed', 'monosyllables', 'unnnt', 'Sssshoo', 'quavering', \"whip's\", 'unbent', 'resigned', 'blows', 'fleeing', 'dodging', 'shadows', 'rocky', 'hid', 'madness', 'glorying', 'mastiff', 'bristling', 'ranted', 'doom', 'enemies', 'gasping', 'draughts', 'incoherent', 'oblivion', 'amused', 'appeasement', 'violence', 'retribution', 'antics', 'disdaining', 'gloom', 'games', 'races', 'paraded', 'grandly', 'attis', 'skeletons', 'paxam', 'dipped', 'arrowheads', 'venom', 'rattlesnakes', 'swift', 'maneuvers', 'firing', 'guns', 'unison', 'indeterminate', 'blankets', 'flew', 'Swan', 'Necklace', 'emulate', 'timidly', 'Yellow', 'Wolf', 'disordered', 'chemistries', 'bolt', 'inaction', 'boiled', 'fermenting', 'juices', 'Alokut', 'challenged', 'matched', 'raced', 'maneuvered', 'abreast', 'cavalry', 'frenziedly', 'regalia', 'preparations', 'combed', 'streaked', 'greased', 'foreheads', 'tails', 'sage', 'hens', 'whitened', 'leggings', 'reflection', 'speeding', 'arrow', 'bow', 'meadow', 'rue', 'admiring', 'envenomed', 'hilltops', 'descend', 'eagle', 'caper', 'cliffs', 'challenge', 'arrogance', 'streaming', 'amulets', 'ripening', 'bellicosity', 'throes', 'shifting', 'Appaloosas', 'Dogs', 'fleet', 'multicolored', 'legion', 'banners', 'drums', 'cacophony', 'accompanying', 'thousand-legged', 'saddles', 'ejaculated', 'hemlocks', 'birch', 'maples', 'quarry', 'unfamiliar', 'stubs', 'suicide', 'expertly', 'root', 'sneakers', 'sliding', 'expanse', 'seating', 'short-cut', 'reedy', 'frogs', 'faded', 'presently', 'vibrant', 'accomplished', 'Paul', 'Easter', 'holidays', 'Evening', 'Dancing', 'attend', 'casually', 'breasts', 'lower-cut', 'inability', 'embarrassment', 'sometime', 'threshold', 'china', 'lamp', 'switch', 'prison', 'indebted', 'Below', 'tunnel', 'fumbled', 'uncovered', 'gripped', 'fearful', 'coursing', 'vessels', 'fabric', 'stains', 'apparent', 'blood-soaked', 'tidiness', 'verge', 'Poldowski', 'owed', 'effete', 'peered', 'wristwatch', 'quarter', 'kneeling', 'tie', 'shoelaces', 'absurdly', 'clumsy', 'strands', 'gauze', 'moist', 'imperative', 'detained', 'freakish', 'cavern', \"Who's\", 'thief', 'Gosh', 'blazer', 'overly', 'knitted', \"Pietro's\", 'finishing', 'float', 'implausibly', 'shaft', 'sunlight', 'oddly', 'partly', \"moment's\", 'paneling', 'inserted', 'eyelids', 'good-bye', 'pounding', 'manage', 'Lincoln', 'parked', 'insolent', 'disdainful', 'Ardmore', 'merest', 'servants', 'born', 'inflection', 'rehearsed', 'analyze', 'Carrie', 'responded', 'restaurant', 'Forget', 'slackened', 'badly', 'disappointing', 'purposeless', 'relinquished', 'Abruptly', 'greatest', 'automatic', 'phrase', 'courtyard', 'tender', 'bumped', 'steadily', 'entry', 'brass', 'screwed', 'kitchenette', 'raw', 'gasp', 'swallows', 'lessen', 'hauled', 'upsets', 'insist', 'sweetheart', 'Happened', 'intoxicated', 'Unfortunately', 'Tony', 'Elliott', 'pinch-hit', \"He'll\", 'nudge', 'postponement', 'bribe', 'double', 'martini', 'relationship', 'Fulbright', 'renovated', 'brick', 'settler', 'clever', 'efficient', 'conformists', 'obnoxious', 'ruffled', 'feathers', 'unreliable', 'irresponsible', 'flyaway', 'jollying', 'Obviously', 'Upstairs', 'showering', 'raindrops', 'pattered', 'grandmother', 'prudent', 'lifetime', 'expressive', \"sister's\", 'Darling', 'mingled', 'blend', 'Leaving', 'supposedly', 'Gregg', 'otherwise', 'jingled', 'scare', \"she'll\", 'defensive', 'Greg', 'depends', 'rescue', 'crises', 'Remembering', 'succession', 'disasters', 'child-cloud', 'youngster', 'fuss', 'accuse', 'looming', 'specter', 'forever-Cathy', 'fiercely', 'piping', 'moon-washed', 'steeped', 'rung', 'stairway', 'brows', 'smoothed', 'muddling', 'worthless', 'clattering', 'overwhelmed', 'junk', 'gushed', 'approaching', 'handkerchief', 'mopping', 'skillfully', 'shape-up', 'groaned', 'gentler', 'grappling', 'outsized', 'armload', 'Scrooge-like', 'lecture', \"cousins'\", 'sympathize', 'limit', \"Cathy's\", 'Lilliputian', 'competing', 'rear', 'flower-scented', 'chilly', 'chillier', 'feeding', 'raffish', 'bobbed', 'gobbled', 'cardinal', 'feathered', 'vestments', 'mate', 'guarding', 'nest', 'sly', 'advantage', 'sentinels', 'abandoning', 'prepare', 'helplessness', 'unscrupulous', 'dump', 'curtains', 'hopping', 'rabbits', 'Goose', 'Mobile', 'hangers', 'neatly', 'fragrant', 'panicky', 'Many', 'Junction', 'phoned', 'sooner', 'napping', 'multitude', 'Methuselah', 'maternal', 'comforting', 'magic', 'Subdued', 'merriest', 'airport', 'fixing', 'hummed', 'voiceless', 'Puzzled', 'flowered', 'forthright', 'silences', 'moth', 'seekingly', 'Shocked', 'elastic', 'cord', 'uncoiling', 'bogey', 'thickened', 'fetching', 'tightened', 'to-and-fro', 'abrupt', \"hall's\", 'purple-black', \"cowbirds'\", 'Cowbird', 'trudged', 'package', \"aunt's\", \"uncle's\", 'Unlike', 'traveling', 'shortest', 'stays', 'Unimpressed', 'plopped', 'disbelieving', 'Esperanza', 'show-offy', 'lifeguards', 'swirling', 'pronounced', 'sherbet-colored', 'mint', 'deserted', 'Eats', 'jiggling', 'jaggedly', \"water's\", 'lumpy', 'trailing', 'dragon', 'Swiss', 'belt', 'corkscrew', 'Canute', 'knight', 'Round', 'Table', 'Sir', 'Brave', 'slaying', 'banshees', 'vampires', 'witches', 'warty', 'astronaut', 'intrepid', 'persecuted', 'fearless', 'scary', 'Tomorrow', 'thump', 'signals', 'radar', 'Son', 'pig', 'cords', 'limping', 'dumb', 'nut', 'polio', 'bedside', 'spilled', 'probly', 'lick', 'Fatso', 'bedtime', 'Strength', 'zip', 'unlaced', 'wadded', 'stripped', 'trunks', 'Goolick', 'goooolick', 'creaked', 'gull', 'dignified', 'bored', 'wedged', 'crawled', 'Watch', 'straps', 'shouders', 'sore', 'boil', \"squatter's\", 'rights', 'Kansas-Nebraska', 'Britches', 'Mark', 'Peters', 'Fifth', 'kinds', 'aleck', 'ripping', 'unconsciously', 'imitating', \"Victoria's\", 'holier-than-thou', 'horrified', 'momentum', 'mama', 'wop', 'wops', 'unimpressed', 'obliged', 'jeans', 'Dingy-looking', 'barber', 'pole', 'spouted', \"cane's\", 'handy', \"Someone's\", 'zoooop', 'snag', 'crook', 'bowl', 'fly', 'bannnnnng', 'stooooomp', 'licking', 'stubby', 'plume', 'collie', 'wire-haired', 'terrier', 'unique', 'DiMaggio', 'tranquilizers', 'braver', 'lately', 'Last', 'jag', 'gotta', 'Squint', 'mornings', 'thereafter', 'blister', 'emerald', 'necklace', 'undying', 'picks', 'nuts', 'Naturally', 'curly', 'runs', 'Personally', 'prefer', 'Continent', 'continent', 'Name', 'pugh', 'beaches', 'scraped', 'Pugh', 'Camels', 'Tripoli', 'harelips', 'Near', 'Galway', 'tinkers', 'caravans', 'gypsies', 'Jerez', 'Really', 'yawn', 'Artfully', 'Cross', 'Korean', 'War', 'spit', 'personally', 'IQ', '141', 'currently', 'Mushr', 'Ozon', 'encyclopedia', 'schnooks', 'Chinaman', 'Tooth-hurty', 'Encouraged', 'imitated', 'described', 'decorated', 'pineapple', 'cherries', 'snuck', 'gumming', 'stumpy', 'panting', 'thirst', 'lagoon', 'staggering', 'Say', 'comic', 'admiringly', 'Either', 'cod', 'salmon', 'sharks', 'bones', 'shrimp', 'encylopedia', 'string', 'Boy', 'Willie', 'Mays', 'outfield', 'je', 'ne', 'quok', 'daydreamed', 'splashed', 'buckets', 'beseech', 'thee', 'sprayed', 'raisin', 'bib', \"Daddy's\", 'Francisco', \"one-o'clock\", 'keen', 'Children', 'organized', 'nap', 'noticing', 'sheets', 'Kool-Aid', 'gagging', 'Sweating', 'drowning', \"afternoon's\", 'Oakmont', 'miswritten', 'heartless', 'prematurely', \"Hadn't\", 'forty-seven', 'twenty-five', 'appalling', 'deserve', 'adult', 'realistic', 'rebound', 'reproach', '1936', 'odds', 'prettiest', 'brightest', 'Allegheny', 'handsomer', 'brighter', 'Pittsburgh', 'seasons', 'Golf', 'goggle-eyed', 'admiration', 'debs', 'claimed', 'skiing', 'crinkles', 'entity', 'Conneaut', \"lovers'\", 'spats', 'heal', 'Dillinger', 'First', \"Cooper's\", 'aggressive', 'thinkers', 'specifically', 'someplace', 'Baltimore', \"Horne's\", 'underestimate', 'giggled', 'bridesmaids', 'choking', 'myth', 'skirts', 'drifted', 'command', 'less-dramatic', 'Sewickley', 'Fox', 'reduced', 'method', 'grindstone', 'Ben', '1938', 'cutest', 'fontanel', 'nary', 'continuing', 'Sally', '1940', '1944', 'dizziness', 'uselessness', 'shuffled', 'missionary', 'Webber', 'eldest', 'elder', 'donated', 'avoid', 'bosoms', 'afterward', 'oversubscribed', 'missionaries', 'bargain', 'whimpering', 'bends', 'overgenerous', 'assignment', 'heir', 'apparency', 'Coopers', 'socially', 'hairpin', 'mourning', 'bites', 'luncheon', 'Le', 'Mont', \"could've\", 'bone-deep', 'sorrow', 'ambitious', 'undo', 'trembled', 'brink', 'settlement', 'soothe', 'capable', 'capturing', 'Years', 'struggled', 'wonders', 'Alloy', 'departments', 'MacIsaacs', 'alloy', 'division', 'Carnegie-Illinois', 'Stuart-family', 'Reuben', 'Pittsburghers', 'sticking', 'dragooned', 'director', 'S.', 'M.', 'jolly', 'factors', 'insiders', 'weighed', 'terms', 'incentive', \"John'll\", 'independent', 'directors', 'agreement', 'Furnaces', 'subordinate', 'advancement', 'meteoric', 'vice', 'biography', 'baby-sitter', 'worth', 'obligated', 'shakily', 'answers', 'frantic', \"Thom's\", \"everything's\", 'Wondering', 'seconds', 'faltered', 'thirteen', 'sitters', 'reliable', 'beautifully', 'ragged', 'spreads', 'law', 'scooted', 'strike', 'hissed', 'thickly', 'lurched', 'raked', 'grabbed', 'insulted', 'takes', 'simmer', 'Tea', 'forgive', 'dime', \"Francie's\", 'televison-record', 'owe', 'grubby', 'filth', 'traipsing', 'curtly', 'collapsed', 'snoring', 'Whether', 'Thankful', 'attending', 'grease', 'confessed', 'pits', 'repaired', 'Seems', 'hired', 'repair', 'promises', 'blew', 'stack', 'Things', 'deserved', 'breaks', 'necks', 'owing', 'received', 'evicted', 'Worst', 'stranded', 'fretted', 'brightened', 'candle', 'embarrassed', 'pleading', 'Straightened', 'flatly', 'uncomfortable', 'darn', 'snobs', 'finance', 'crawl', 'dishes', 'groceries', 'Virus', 'infection', 'lazy', 'no-good', 'alibis', 'excuses', 'Wendell', 'black-balled', 'grocery', 'sewed', 'punks', \"they'll\", 'encouraging', 'July', 'stifling', 'intuition', 'based', 'nine-to-five', 'five-days-a-week', 'medicine', 'shaving', 'sneezed', 'orphanage', 'boarding-home', 'fonder', 'Especially', 'Growing', 'intern', 'lightened', 'heaviness', 'Should', 'interns', 'Ishii', 'resident', 'Medicine', 'sicker', 'post-operative', 'Got', 'lulu', 'alcoholics', 'segregated', 'charity', 'focus', 'forming', 'alcoholism', 'puffy', 'marred', 'tell-tale', 'veins', 'drinkers', 'two-colored', 'bleached', 'unfortunate', 'resemblance', 'forcing', 'disquiet', 'knocked', 'AA', 'describe', 'ounce', 'relaxes', 'chat', 'gladly', 'drawn-back', 'bloodspots', 'spotting', 'emotion', 'negative', 'Papanicolaou', 'vaginal', 'gross', 'lab', 'Late', 'results', 'D.', 'C.', 'hysterectomy', 'intraepithelial', 'situ', 'wryly', 'tells', 'badgering', 'Wants', 'booze', 'grimaced', \"Bancroft's\", 'knob', 'dozing', 'disregarded', 'dilatation', 'curettage', 'proves', 'definite', 'difficulty', 'liquor', 'surgery', 'Mostly', 'coverlet', 'secrets', \"patient's\", 'disregarding', 'protests', 'guinea', 'pigs', 'learn', 'cultivated', 'strongly', 'assure', 'gaped', 'intently', 'half-smile', 'Surprised', 'Guess', 'heart-stopping', 'stammered', 'goodness', 'echoed', 'mockingly', 'Think', 'introduce', 'alcoholic', 'blackout', 'favors', 'alike', \"what's\", 'pathetic', 'disappear', 'caresses', 'tricks', 'ultimate', 'pressures', 'squeezed', 'breathed', 'anesthetic', 'yielding-Mediterranian-woman-', 'soothed', '230', 'drunker', 'metal-tasting', 'suck', 'aqua-lung', 'swim', 'chuckled', 'hazy', 'concentrated', 'sorted', 'duds', 'bellyfull', 'Wild', 'kicks', 'dig', 'twenty-one', 'Orly', 'Rhine-Main', 'April', 'mist-like', 'pocketful', 'grounded', 'Champs', 'Elysees', 'dazzled', 'machines', 'Bugatti', 'Farina', 'coachwork', 'chassis', 'Swallow', 'A40-AjK', 'Mercedes', 'Arc', 'de', 'Triomphe', 'Tour', \"d'Eiffel\", 'yokel', \"Maxim's\", 'Claire', 'feed', 'Handsome', 'soldier', 'assuaged', 'urgency', 'Madame', 'noblesse', 'oblige', 'yeah', 'nymphomaniac', 'Toward', 'waxed', 'philosophical', 'analogies', 'mistake', 'sweep', 'Panther', 'Pils', 'switched', 'Tuborg', 'crocked', 'orbit', 'Capricorn', 'Cancer', 'Elemental', 'debauchery', 'reconciled', 'flop', 'binge', 'therapeutic', 'Sometime', 'snowing', 'snowy', 'dim', 'flakes', 'Pretty', 'poise', 'posture', 'drab', 'propriety', 'unuttered', 'bleak', 'unhappiness', 'profoundly', 'derriere', 'crumble', 'snorted', 'chuckle', 'tiredness', 'infinite', 'reinforce', 'gambit', 'Remy', 'cork', 'Non', 'non', 'lug', 'tart', 'drowsing', 'Allons', 'rested', 'rime', 'melted', 'absolute', \"night's\", 'melancholy', 'vital', 'brightly', 'tousled', 'make-up', \"J'ai\", 'faim', 'wheeled', 'presenting', 'dangerously', 'scald', 'filter', 'tremendous', 'amount', 'hard-boiled', 'garlic', 'Suzanne', 'comfortably', 'remarked', 'Um', 'grunted', 'sipping', 'les', 'putains', 'immense', 'sentence', 'Shall', 'enthusiasm', 'immediately', 'bullet', 'Metro', 'cabaret', 'steamed', 'mussels', 'sommelier', 'magnum', 'steely', 'Jeroboam', 'humorous', \"mind's\", 'responding', 'exhaustingly', \"shores'\", 'powers', 'telescoped', 'plains', 'aqueducts', 'tombs', 'cypress', 'Appian', 'Way', 'Arch', 'Constantine', 'Moreover', 'nursing', 'aunt', 'lengthy', 'illnesses', 'England', 'France', 'Germany', 'Switzerland', 'rendered', 'tonics', 'conceived', 'lamplight', 'Cars', 'taxis', 'buses', 'motorscooters', 'swerving', 'perilously', 'groups', 'German', 'cameras', 'attached', 'Glad', 'outdoor', 'unloading', 'potted', 'placing', 'banked', 'rows', 'shrubs', 'myriad', 'bud', 'ranged', 'fuchsia', 'palest', 'Marvelous', 'portly', 'well-bred', 'Halfway', 'Other', 'banisters', 'parapets', 'Mediterranean', 'underside', 'insides', 'churches', 'Content', 'excited', 'contracted', 'dismay', 'script', \"t's\", \"l's\", 'inclined', 'wobble', 'slope', 'post', 'Alabama', 'mailed', 'next-door', 'neighbor', 'Piazza', 'di', 'Spagna', 'President', 'Republic', 'Inside', 'concerning', 'illness', 'devoted', 'aging', 'immemorial', 'Montgomery', 'sweetpeas', 'mother-of-pearl', 'coveted', 'sigh', 'refolded', 'alas', 'reminiscence', 'weakening', 'values', 'diapers', 'gentility', 'principle', 'lest', 'fraternity', 'pursued', 'mammas', 'aroused', 'uttermost', 'cramp', 'hobbled', 'bathrobe', 'footstool', 'clenched', 'heroically', 'monkey', 'kittens', 'kettle', 'steaming', 'enamelled', 'pan', 'treat', 'hiccups', 'Drop', 'unamused', 'round-eyed', 'woe', 'parting', 'Robbie', 'Beryl', 'imposition', 'Rosie', 'populated', 'Edna', 'Whittaker', 'meets', 'crumbling', 'makeshift', 'sneezing', 'uncertainly', 'unnaturally', 'tray', 'cameos', 'pens', 'papal', 'portraits', 'borders', 'Carrozza', 'escape', 'landing', 'vacant', 'banister', 'vendor', 'well-dressed', 'ascending', 'cast', 'policeman', 'viewed', 'interlude', 'hinting', 'teachers', 'September', 'crisis', 'Balzac', 'Dickens', 'Stendhal', \"Mother's\", 'operation', 'twenty-six', 'volumes', 'Dinsmore', 'arranging', 'Mi', 'diapiace', 'ma', 'insomma', 'indicated', 'stealer', '4000-plus', \"nobody'd\", 'bothered', 'good-living', 'truthfully', 'Sue', 'Much', \"Lucille's\", 'Honest', 'compete', 'trucker', 'carpentry', 'February', 'delivery', 'masculine', 'providing', 'Neither', 'submit', 'penniless', 'helpless', 'Against', \"folks'\", 'crackling', 'vices', 'jockey', 'drinks', 'smokes', 'ticking', 'items', 'swears', 'whoever', 'examined', 'noncommittally', 'scheming', 'Devil', 'Route', '10', 'clutches', 'blushed', 'sexual', 'triggered', 'fierceness', 'Astonishingly', 'summertime', 'fields', 'crickets', 'serenaded', 'huh', 'blissful', 'fulfilling', 'witnesses', 'certificate', 'Infinite', 'contentment', \"Idiot's\", 'upbringing', 'resisted', 'passes', \"Wouldn't\", 'insure', \"Johnnie's\", 'denied', 'contribute', 'doubts', 'petted', 'longest', 'behaved', 'emotionally', 'mere', 'rescued', 'husband-stealer', 'wondrous', 'fitting', 'union', 'Uh-huh', 'Convention', 'eastern', 'represent', 'mixed', 'emotions', 'Faneuil', 'cousin', 'convention', 'introduced', 'Rhode', \"someone's\", 'anxiously', 'twin', 'pumpkin', 'Schmalma', 'shrilly', 'alcohol', 'disguised', 'ginger', 'ale', \"'most\", 'snugly', \"C'mon\", 'lubricated', 'all-American-boy', 'Astronaut', 'vitamin', 'Breakfast', 'Eden', 'two-burner', 'pelting', 'lunatic', 'arrangement', 'Share', 'units', 'bachelor-type', 'eatables', 'curving', 'bloomed', 'haze', 'names', 'magenta', 'downed', 'Lots', 'vitamin-and-iron', 'compound', 'capsule', 'comment', 'beriberi', 'cure', 'Wayne', 'kissing', 'piling', 'ailment', 'confirmed', 'distrust', 'scalded', 'encouragement', 'shy', 'Woods', 'blinds', 'dawning', 'poker', 'secretaries', 'Meanwhile', 'sandy', 'puddles', 'prescription', 'mockery', 'odds-on', 'develop', 'awesome', 'black-and-yellow', 'polka-dotted', 'slicker', 'Three-day', 'natives', 'filthy', 'Pyhrric', 'Fine', 'Thought', 'bake', 'briefly', 'forbore', 'universal', 'weatherproof', 'conviction', 'honeymooning', 'lounging', 'sands', 'unalloyed', 'bliss', 'leafed', 'adventures', 'Seashore', 'Farm', 'Danger', 'agricultural', 'treatment', 'hoof-and-mouth', 'disease', 'cattle', 'hideously', 'illustrated', 'bruising', 'shin', 'choke', 'fuzzy', 'mania', 'dyed', 'climb', 'long-hair', 'according', 'label', 'explanation', 'Bermuda', 'Said', 'permeates', 'squirt', \"Kissin'\", 'Kare', 'handwriting', 'dainty', 'evidence', 'Sorry', 'swiping', 'onions', 'Ugh', 'Must', 'idly', 'fast-frozen', 'guided', 'obscure', 'horse-blanket', 'plaid', 'splashy', 'Pink', 'grown-up', 'clinging', 'infancy', 'toilsome', 'laden', 'clattery', 'mops', 'brushes', 'pails', \"rain's\", 'scorcher', 'scrutinized', 'warn', \"kind's\", 'inviting', 'description', 'Run-down', 'iron-poor', 'Frail', 'feeble', 'blazing', 'varicolored', 'properties', \"four-o'clock\", 'gaudy', 'mist', 'floodlit', 'punctuated', 'refrigerators', 'retreat', 'Cape', \"flower's\", 'transferred', 'M-m-m', 'propped', 'postscript', 'scrap', 'Gee', 'redheads', 'rental', 'browny', 'blondes', 'bikinis', 'preponderance', 'tank', 'Up', 'dune', 'stool', 'bulky', 'turtle-neck', 'sweaters', 'not-so-pale', 'moonlit', 'filched', 'Use', \"goodness'\", 'Sympathy', \"Vivian's\", 'slopping', 'offering', 'peel', 'pajama', 'subside', 'quantities', 'Correspondence', 'glycerin', 'whitens', 'Broiled', 'Puny', 'Surviving', 'Wilderness', 'Speedy', 'Canoe', 'Also', 'canoe', 'overhand', 'innings', 'fifth', \"Anniston's\", 'smacked', \"Riverside's\", 'redheaded', 'relay', 'punches', 'uproar', 'fighters', 'resumed', 'cursed', 'eyeing', 'sonny', 'infuriated', 'Mind', 'goddamn', 'sixth', \"fielder's\", 'rounding', 'heading', 'tagged', 'straddling', 'rammed', \"catcher's\", 'physician', 'injured', 'nearer', 'quivering', 'ram', 'calculated', 'pitching', 'powerfully', 'hander', 'eighth', 'hefty', \"batter's\", 'Hit', 'shortstop', 'bastards', 'Haydon', 'pounded', 'rubber', 'flat-footed', 'unhurried', 'infield', 'Fights', 'squelched', 'helluva', 'team-mate', 'undressing', 'inches', \"Eddie's\", 'unconditional', 'belong', 'confirm', 'Sit', 'steering', 'pro-ball', 'Dazed', 'fielding', 'bases', \"Baseball's\", 'cinch', 'ramming', 'puts', 'sport', 'goddamit', 'ribbons', 'league', 'Someday', \"Springfield's\", 'Talk', 'Ask', 'Springfield', 'Atta', 'outfielders', 'insularity', 'by-ways', 'fascinate', 'Augustine', 'Aquinas', 'Lao-tse', 'Confucius', 'Mencius', 'Suzuki', 'Hindu', 'tomes', 'Krishnaists', 'socio-archaeological', 'papers', 'Zend-Avesta', 'Indian', 'entitled', 'Grinned', 'gloomily', 'laughter', 'formal', 'sixteen', 'substituted', 'fond', 'graduated', 'foundation', 'remoteness', 'thorough', 'bookish', 'lore', 'literature', 'politics', 'awarded', 'practicing', 'sailing', 'distrusted', 'buying', 'shorten', 'relax', 'incarcerated', 'dusted', 'sped', 'plucked', 'wrought', 'resemble', 'instrument', 'stole', 'gusty', 'similar', 'Adams', 'succeeded', 'unusually', 'frankness', 'zest', 'envied', 'occasional', 'participate', 'tea-drinking', 'complied', 'random', 'gatherings', 'flippant', 'superficial', 'pointless', 'persons', 'students', 'judging', 'popular', 'foods', 'equation', 'Zen', 'philosophy', 'modifier', 'Soba', 'udon', 'noisily', 'Sushi', 'Sashimi', 'Asked', 'Witter', 'irritation', 'Lovers', 'Mound', 'Gompachi', 'Komurasaki', 'parkish', 'concluded', 'Anyhow', 'potatoes', 'Kanto', 'bothersome', 'pull', 'Twenty-two', 'twenty-three', 'colder', 'tiniest', 'reflect', 'version', 'ascetic', 'returning', 'waters', 'gate', 'spigots', 'caged', 'incarnation', 'creature', 'overwhelmingly', 'likened', 'limpid', 'fountain-falls', 'familiarity', 'Into', 'hundred-yen', 'urge', 'bronze', 'priest', 'Instead', 'stare', 'rigidly', 'fascination', 'metallic', 'scraping', 'stillness', 'severe', 'emptied', 'pockets', 'coins', 'hurried', 'stairways', 'virus', 'process', 'reminder', 'consciousness', 'lingered', 'inexplicable', 'elements', 'reality', 'resolved', 'farm', 'snow-fence', 'wheels', 'loaded', 'multi-colored', 'graceful', 'throttle', 'fright', 'idle', 'movement', 'tubular', 'interlaced', 'jet-black', 'astonishing', 'unthinkable', 'fire-colored', 'lumps', 'forepart', 'fieldmice', 'satin', 'newly-plowed', 'target', 'shone', 'sun-burned', 'crumbled', 'paleness', 'dusty', 'uncolored', 'triangular', 'plowshares', 'scouring', 'stamping', 'ferocity', 'frenzied', 'impatience', 'fierce', 'hunched-up', 'determination', 'whipped', 'lumbering', 'halt', 'frenetic', 'savagery', 'glimmer', 'remnant', 'ache', 'fumes', 'crushed', 'vanish', 'mount', 'bubbly', 'finely-spun', 'effeminate', 'eyelashes', 'brushlike', 'newly-scrubbed', 'resembled', 'splintered', 'bitten', 'poisonous', 'foolishly', 'gray-looking', 'beautifully-tapered', 'flattened', 'vise', 'splayed', 'wrenched', 'thickest', 'colored', 'crystals', 'ice-feeling', 'spasm', 'muscles', 'wrenches', 'tool-kit', 'gleeful', 'puzzled', 'brute', 'nasty', 'beast', 'slap', 'wipe', 'insolence', 'glee', 'unglued', 'tidings', 'Tax', 'alma', 'mater', 'snapper', 'enrolled', 'credentials', 'transcript', 'references', 'boards', 'applicants', 'co-operate', 'qualifications', 'enroll', 'merchant', 'Gone', 'reunions', '1935', 'old-grad-type', 'Alcorn', \"Pete's\", 'rejected', 'football', 'B', \"A's\", 'math', 'temples', 'chemistry', 'Getting', 'politicking', 'conscience', 'recommend', 'candidate', 'rugged', 'Height', \"6'\", 'Weight', '160', 'Health', 'excellent', 'astronomy', 'geology', 'enrolling', 'keyboard', 'ruining', 'Venetian', 'bestowed', 'bucket', 'commending', 'eyebrow', 'amusement', 'soul-searching', 'participated', 'high-school', 'activities', 'civic', 'flurry', 'shoved', 'unsealed', 'omitted', 'whistled', 'locked', 'gasser', 'rounded', 'pre-packed', 'State', 'rocket', 'messing', 'Real', 'Yale', 'thinning', 'unease', 'Weakness', 'Limited', 'darned', 'Martini', 'tasted', 'meal', 'salad', 'surcease', 'ease', 'sprinkle', 'navy-blue', 'shag', 'woolly', 'Board', 'Work', 'midweek', 'eight', 'commencement', 'Carefully', 'loudly', 'Chairman', \"Partlow's\", 'proverbial', 'pie', 'methodically', 'Burke', 'straight-A', 'antisocial', 'lone', 'completely', 'one-sided', 'Girls', 'parent', 'frayed', 'coffeepot', 'tattered', 'book-lined', 'dilapidated', 'Wrong', \"Dave's\", 'motives', \"Here's\", 'belligerence', 'Already', \"Anne's\", 'acting', 'Daley', 'basketball', 'play-off', 'remains', 'Astronomy', 'stars', 'Geology', 'fishpond', 'suggest', 'chess', 'Music', 'season', 'rock-and-roll', 'combo', 'solos', 'guitar', 'Rich', 'sax', 'solo', 'shine', 'Beethoven', 'mollified', 'brutally', 'loosen', 'padding', 'document', 'ham-radio', 'sale', 'curb', 'applying', 'towel', 'hemming', 'guild', 'banker', 'conform', 'leader', 'well-worn', 'chip', 'qualities', 'Give', 'Army', 'ninety-nine', 'generals', 'mailbox', 'arguing', 'monstrous', 'proportions', 'cotton', 'ruled', 'Lying', 'messy', 'negligent', 'strewn', 'fun', 'kidding', 'trail', 'littered', 'rug', 'assertive', 'grinning', 'refusal', 'stoop', 'petals', 'seed', 'catalogues', 'referred', 'flower', 'contain', 'patina', 'veining', 'reveal', 'column', 'vertebrae', 'magnificently', 'unyielding', 'segments', 'bone', 'cracking', 'noises', 'stem', 'tulip', 'bluntly', 'seriously', 'leisure', 'beforehand', 'delights', 'emotional', 'clash', 'invigorating', 'tenth', 'teasing', 'pat', 'superhuman', 'neatness', 'incongruity', 'mole', 'Quietly', 'foil', 'doorbell', 'Ten', 'cross', 'bend', \"Bill's\", 'deposited', 'strained', 'glissade', 'instructed', 'ellipsis', 'topic', 'Brainards', 'aback', 'contrary', 'weaken', 'fumed', 'adamant', 'raving', 'titters', 'whisperings', 'scabrous', 'unclean', 'gossiping', 'lowering', 'hoarseness', 'crouched', 'Afterwards', 'apologized', 'annoyed', 'pallid', 'resolution', 'About', 'newly', 'Angrily', 'delayed', 'preferably', 'lasted', 'deciding', 'postpone', 'mercy', 'These', 'stray', 'inspection', 'ever-present', 'visitors', 'blot', \"bedroom's\", 'countenance', 'Quite', 'lighthearted', 'witty', 'gaily', 'anecdote', 'frothy', 'deceptive', 'merriment', 'snort', 'mock', 'ritual', 'relations', 'fatal', 'scattering', 'possessions', 'drawers', 'impinge', 'Bizarre', 'bizarre', 'Six', 'Pursuing', 'trapped', 'Day', 'dilemma', 'frantically', 'seeking', 'exit', 'Alternately', 'periods', 'hostile', 'defeatism', 'sullenly', 'morosely', 'simplicity', 'occurred', 'frenzy', 'Beside', 'Instantaneously', 'immeasurable', 'panties', 'alongside', 'particular', 'humiliation', 'Furthermore', 'maneuver', 'endlessly', 'slip', 'brassiere', 'girdle', 'traversed', 'installment', 'rumpled', 'childishness', 'looseness', 'untouched', 'disciplined', 'unruly', 'underclothes', 'linked', 'visibly', 'magnificence', 'virility', 'analyzing', 'scheme', 'exalted', 'fanaticism', 'nylon', 'tenderly', 'focused', 'Slowly', 'reasoning', 'grasped', 'implications', 'occurring', 'Extending', 'inch', 'swiftly', 'unsteady', 'breakfasted', 'refer', \"That'll\", 'resurgence', 'industrialist', \"nibs'\", 'racket', 'glory', 'inadvertent', 'agency', 'brother-in-law', \"Hershey's\", 'draft', 'Eddyman', 'responsible', 'eminence', 'detached', 'estate', 'skyscraper', 'provide', 'Plastic', 'skeleton', 'sorts', 'undergone', 'vicissitudes', 'engaged', 'Shortly', 'proliferation', 'bold', 'exposed', 'suggestions', 'prospering', 'influenced', 'involvement', 'familial', 'loyalty', 'aid', 'prodded', \"Joan's\", 'competency', 'factor', 'eventual', 'disposal', 'unquestionably', 'entwined', 'likely', 'richly', 'Whatever', 'factory', 'ceramics', 'experimentally', 'high-speed', 'calculators', 'political', 'additional', 'worlds', 'conquer', 'Heavy', 'industry', 'slanted', 'inexhaustible', 'coffers', 'attracted', 'Auto', 'Company', 'medium-sized', 'manufactured', 'four-wheel-drive', 'vehicles', 'off-road', 'over-large', 'misguided', 'optimism', 'Cursed', 'dissatisfied', 'stockholders', 'amalgamation', 'mergers', 'consequences', \"Herberet's\", \"Allstates'\", 'folly', 'airplane', 'sub-assembly', 'tanks', 'missiles', 'ordnance', 'desiring', 'Freed', 'complaisant', 'broader', 'overall', 'administration', 'corporate', 'structure', 'wider', 'enchanted', 'proposition', 'Chrysler', 'acquainted', 'hardware', 'Air', 'Force', 'technical', 'Missiles', 'grabs', 'manned', 'affects', 'procurement', 'transports', 'revolutionized', 'airline', 'compound-engine', 'planes', 'companies', 'competition', 'assembled', 'settles', 'bound', 'pages', 'assures', 'raising', 'legal', 'exhibit', 'deadly', 'dampening', 'elderly', 'caution', 'appreciate', 'lukewarm', 'entering', 'fray', 'cudgels', 'prospect', 'mix', 'commercial', 'patriarchy', 'aide', 'Hamilton', 'prevailed', 'devoting', 'swaying', 'dismissed', 'project', 'unwilling', 'interfere', 'transfers', 'hyphenated', 'Clay', 'backing', 'deficit', 'beset', 'deliver', 'elephant', 'Stock', 'abundance', 'Confronted', 'plumped', 'drastic', 'liquidation', 'summoned', 'bind', 'addition', 'defects', 'closeted', 'motor', 'existence', \"A-Z's\", 'set-up', 'exposure', 'conceptions', 'coincidental', 'failure', 'out-dated', 'small-car', 'manufacturer', 'dimensions', 'broach', 'Initially', 'passenger', 'discouraging', 'models', 'founding', 'nationwide', 'dealerships', 'cheaply', 'presses', 'dies', 'precisely', 'hurdle', 'insurmountable', 'investigate', 'marshal', 'statistics', 'arguments', 'Taking', 'dubious', 'alternative', 'breathtaking', 'frank', 'resolve', 'guarantee', 'audience', 'advocating', 'publicly', 'Forgive', 'attended', 'staff', 'presiding', 'incredulity', 'listeners', 'animosity', 'quarrels', 'transact', 'larger', 'low-priced', 'bucking', 'old-fashioned', 'sell', 'economy', 'merit', 'romance', 'snobbery', 'European', 'woo', 'consumer', 'bludgeon', 'chromed', 'excess', 'seduction', 'dealers', 'ready-made', 'steam', 'yachts', 'Georgian', 'bloated', 'too-expensive', 'Allstates-Zenith', 'debate', 'raged', 'Financing', 'emerged', 'obstacle', 'contributed', 'maximum', 'underwrite', 'department', 'risk', 'risky', 'basis', 'capital', 'Heads', 'instinctively', 'onus', 'recriminations', 'broadcast', 'availing', 'in-laws', 'Sweat', 'forehead', 'disquietude', 'Across', 'saluted', 'jubilantly', 'encircled', 'forefinger', 'Spike-haired', 'burly', 'red-faced', 'decked', 'horn-rimmed', \"officers'\", 'expect', 'episode']\n",
            "                  0    1    2    3    4    5    6    7    8    9 \n",
            "       English    0  185  525  883  997 1166 1283 1440 1558 1638 \n",
            "German_Deutsch    0  171  263  614  717  894 1013 1110 1213 1275 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 2D : Study of tagged corpora with methods like tagged_sents, tagged_words."
      ],
      "metadata": {
        "id": "teOgwb6qj_lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRV6GUPLlAuo",
        "outputId": "c6b96c97-bfd7-43b3-8741-621caffa93c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1eStz28lC-b",
        "outputId": "68ce26ca-284d-42a6-c961-cd22e90f732f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "\n",
        "para = \"Hello! My name is Beena Kapadia. Today you'll be learning NLTK.\"\n",
        "sents = tokenize.sent_tokenize(para)\n",
        "print(\"\\nSentence Tokenization\\n===================\")\n",
        "for sent in sents:\n",
        "    print(sent)\n",
        "\n",
        "print(\"\\nWord Tokenization\\n===================\")\n",
        "for sent in sents:\n",
        "    words = tokenize.word_tokenize(sent)\n",
        "    print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE6SX3hzkKZC",
        "outputId": "1e83d809-011e-44f1-e76e-914aec13c538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Tokenization\n",
            "===================\n",
            "Hello!\n",
            "My name is Beena Kapadia.\n",
            "Today you'll be learning NLTK.\n",
            "\n",
            "Word Tokenization\n",
            "===================\n",
            "['Hello', '!']\n",
            "['My', 'name', 'is', 'Beena', 'Kapadia', '.']\n",
            "['Today', 'you', \"'ll\", 'be', 'learning', 'NLTK', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 2E : Write a program to find the most frequent noun tags."
      ],
      "metadata": {
        "id": "uF5XDoXrlJkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H44-kZfljxu",
        "outputId": "65f1c2e4-5d51-4330-9c0b-b096ce9b9cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import defaultdict\n",
        "\n",
        "text = nltk.word_tokenize(\"Nick likes to play football. Nick does not like to play cricket.\")\n",
        "tagged = nltk.pos_tag(text)\n",
        "print(tagged)\n",
        "\n",
        "# Checking if it is a noun or not\n",
        "addNounWords = []\n",
        "count = 0\n",
        "for word, tag in tagged:\n",
        "    if tag in ['NN', 'NNS', 'NNPS', 'NNP']:\n",
        "        addNounWords.append(word)\n",
        "    count += 1\n",
        "\n",
        "print(addNounWords)\n",
        "\n",
        "temp = defaultdict(int)\n",
        "# Memoizing count\n",
        "for sub in addNounWords:\n",
        "    for wrd in sub.split():\n",
        "        temp[wrd] += 1\n",
        "\n",
        "# Getting max frequency\n",
        "res = max(temp, key=temp.get)\n",
        "\n",
        "# Printing result\n",
        "print(\"Word with maximum frequency: \" + str(res))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMvL3rzflNuj",
        "outputId": "46c6a796-f81b-4f2a-9d37-e3ec66bd355f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Nick', 'NNP'), ('likes', 'VBZ'), ('to', 'TO'), ('play', 'VB'), ('football', 'NN'), ('.', '.'), ('Nick', 'NNP'), ('does', 'VBZ'), ('not', 'RB'), ('like', 'VB'), ('to', 'TO'), ('play', 'VB'), ('cricket', 'NN'), ('.', '.')]\n",
            "['Nick', 'football', 'Nick', 'cricket']\n",
            "Word with maximum frequency: Nick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 2F : Map Words to Properties Using Python Dictionaries code:"
      ],
      "metadata": {
        "id": "33EJZpXEloaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thisdict = {\n",
        "    \"brand\": \"Ford\",\n",
        "    \"model\": \"Mustang\",\n",
        "    \"year\": 1964\n",
        "}\n",
        "\n",
        "print(thisdict)\n",
        "print(thisdict[\"brand\"])\n",
        "\n",
        "print(len(thisdict))\n",
        "print(type(thisdict))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbJWxXTPlwgp",
        "outputId": "c7bc5fb4-7f4a-46e9-e29b-194990dbd88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'brand': 'Ford', 'model': 'Mustang', 'year': 1964}\n",
            "Ford\n",
            "3\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 2G : Study i) DefaultTagger, ii) Regular expression tagger, iii) UnigramTagger"
      ],
      "metadata": {
        "id": "QQTNnCcol6BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i)\tDefaultTagger"
      ],
      "metadata": {
        "id": "laujRtvSmP2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('treebank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp69Xq5YmM3k",
        "outputId": "23918e5c-615a-4462-e876-65ad6c48f88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import DefaultTagger\n",
        "\n",
        "exptagger = DefaultTagger('NN')\n",
        "\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "testsentences = treebank.tagged_sents()[1000:]\n",
        "print(exptagger.evaluate(testsentences))\n",
        "\n",
        "# Tagging a list of sentences\n",
        "import nltk\n",
        "from nltk.tag import DefaultTagger\n",
        "\n",
        "exptagger = DefaultTagger('NN')\n",
        "sentences = [['Hi', ','], ['How', 'are', 'you', '?']]\n",
        "\n",
        "for tagged_sentence in exptagger.tag_sents(sentences):\n",
        "    print(tagged_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdhEblPumAMP",
        "outputId": "0fb25800-b669-484f-aba5-48f3d4061ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-4586b4b65de0>:9: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print(exptagger.evaluate(testsentences))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13198749536374715\n",
            "[('Hi', 'NN'), (',', 'NN')]\n",
            "[('How', 'NN'), ('are', 'NN'), ('you', 'NN'), ('?', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ii) Regular expression tagger"
      ],
      "metadata": {
        "id": "-bhj4qS1mVt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('treebank')"
      ],
      "metadata": {
        "id": "-hp2n8E2mysZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "from nltk.tag import RegexpTagger\n",
        "\n",
        "test_sent = brown.sents(categories='news')[0]\n",
        "\n",
        "regexp_tagger = RegexpTagger([\n",
        "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),     # cardinal numbers\n",
        "    (r'(The|the|A|a|An|an)$', 'AT'),     # articles\n",
        "    (r'.*able$', 'JJ'),                  # adjectives\n",
        "    (r'.*ness$', 'NN'),                  # nouns formed from adjectives\n",
        "    (r'.*ly$', 'RB'),                    # adverbs\n",
        "    (r'.*s$', 'NNS'),                    # plural nouns\n",
        "    (r'.*ing$', 'VBG'),                  # gerunds\n",
        "    (r'.*ed$', 'VBD'),                   # past tense verbs\n",
        "    (r'.*', 'NN')                        # nouns (default)\n",
        "])\n",
        "\n",
        "print(regexp_tagger)\n",
        "print(regexp_tagger.tag(test_sent))\n"
      ],
      "metadata": {
        "id": "U64_iPptmcya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iii)\tUnigramTagger"
      ],
      "metadata": {
        "id": "e-YhgH3nmht5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('treebank')"
      ],
      "metadata": {
        "id": "ItuNsVWPm0Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import UnigramTagger\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Training using the first 10 tagged sentences of the treebank corpus as data.\n",
        "train_sents = treebank.tagged_sents()[:10]\n",
        "\n",
        "# Initializing the tagger\n",
        "tagger = UnigramTagger(train_sents)\n",
        "\n",
        "# Printing the first sentence of the treebank corpus as a list\n",
        "print(treebank.sents()[0])\n",
        "\n",
        "# Printing the tagged result after tagging the first sentence using the tagger\n",
        "print(tagger.tag(treebank.sents()[0]))\n",
        "\n",
        "# Finding the tagged results after training\n",
        "print(tagger.tag(treebank.sents()[0]))\n",
        "\n",
        "# Overriding the context model\n",
        "tagger = UnigramTagger(model={'Pierre': 'NN'})\n",
        "print('\\n', tagger.tag(treebank.sents()[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqwMpxqOmmBG",
        "outputId": "3d4d7ad8-0544-477c-b37b-802a4a45e2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "\n",
            " [('Pierre', 'NN'), ('Vinken', None), (',', None), ('61', None), ('years', None), ('old', None), (',', None), ('will', None), ('join', None), ('the', None), ('board', None), ('as', None), ('a', None), ('nonexecutive', None), ('director', None), ('Nov.', None), ('29', None), ('.', None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Practical 2H : Find different words from a given plain text without any space by comparing this text with\n",
        "#  a given corpus of words. Also find the score of words."
      ],
      "metadata": {
        "id": "qEBp6TKTm2sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import with_statement  # Importing with statement for reading file\n",
        "import re  # Regular expression\n",
        "\n",
        "words = []  # Corpus file words\n",
        "testword = []  # Test words\n",
        "ans = []  # Words matches with corpus\n",
        "\n",
        "print(\"MENU\")\n",
        "print(\"-----------\")\n",
        "print(\" 1 . Hash tag segmentation \")\n",
        "print(\" 2 . URL segmentation \")\n",
        "print(\"enter the input choice for performing word segmentation\")\n",
        "\n",
        "choice = int(input())\n",
        "\n",
        "if choice == 1:\n",
        "    text = \"#whatismyname\"  # Hash tag test data to segment\n",
        "    print(\"input with HashTag\", text)\n",
        "    pattern = re.compile(\"[^\\w']\")\n",
        "    a = pattern.sub('', text)\n",
        "elif choice == 2:\n",
        "    text = \"www.mynameispawan.com\"  # URL test data to segment\n",
        "    print(\"input with URL\", text)\n",
        "    a = re.split('\\s|(?<!\\d)[,.](?!\\d)', text)\n",
        "    splitwords = [\"www\", \"com\", \"in\"]  # remove the words which is containg in the list\n",
        "    a = \"\".join([each for each in a if each not in splitwords])\n",
        "else:\n",
        "    print(\"wrong choice...try again\")\n",
        "    exit()\n",
        "\n",
        "print(a)\n",
        "\n",
        "for each in a:\n",
        "  testword.append(each)  # Test word\n",
        "\n",
        "test_lenth = len(testword)  # Length of the test data\n",
        "\n",
        "# Reading the corpus\n",
        "with open('/content/words.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    words = [(e.strip()) for e in lines]\n",
        "\n",
        "\n",
        "def Seg(a, lenth):\n",
        "    ans = []\n",
        "    for k in range(0, lenth + 1):  # this loop checks char by char in the corpus\n",
        "        if a[0:k] in words:\n",
        "            print(a[0:k], \"-appears in the corpus\")\n",
        "            ans.append(a[0:k])\n",
        "            break\n",
        "    if ans != []:\n",
        "        g = max(ans, key=len)\n",
        "        return g\n",
        "\n",
        "\n",
        "test_tot_itr = 0  # each iteration value\n",
        "answer = []  # Store the each word contains the corpus\n",
        "Score = 0  # initial value for score\n",
        "N = 37  # total no of corpus\n",
        "M = 0\n",
        "C = 0\n",
        "\n",
        "while test_tot_itr < test_lenth:\n",
        "    ans_words = Seg(a, test_lenth)\n",
        "    if ans_words != 0:\n",
        "        test_itr = len(ans_words)\n",
        "        answer.append(ans_words)\n",
        "        a = a[test_itr:test_lenth]\n",
        "        test_tot_itr += test_itr\n",
        "\n",
        "Aft_Seg = \" \".join([each for each in answer])\n",
        "# print segmented words in the list\n",
        "print(\"output\")\n",
        "print(\"---------\")\n",
        "print(Aft_Seg)  # print After segmentation the input\n",
        "\n",
        "# Calculating Score\n",
        "C = len(answer)\n",
        "score = C * N / N  # Calculate the score\n",
        "print(\"Score\", score)\n"
      ],
      "metadata": {
        "id": "u20ELBDdndEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a words.txt file then paste the below words and upload in the left side content folder\n",
        "\n",
        "check\n",
        "domain\n",
        "big\n",
        "rocks\n",
        "name\n",
        "cheap\n",
        "being\n",
        "human\n",
        "current\n",
        "rates\n",
        "ought\n",
        "to\n",
        "go\n",
        "down\n",
        "apple\n",
        "domains\n",
        "honesty\n",
        "hour\n",
        "follow\n",
        "back\n",
        "social\n",
        "media\n",
        "30\n",
        "seconds\n",
        "earth\n",
        "this\n",
        "is\n",
        "insane\n",
        "it\n",
        "time\n",
        "what\n",
        "is\n",
        "my\n",
        "name\n",
        "let\n",
        "us\n",
        "go\n",
        "pawan\n",
        "name"
      ],
      "metadata": {
        "id": "Euu5BBIl5J0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 3A : Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms"
      ],
      "metadata": {
        "id": "6yWsKyEO5gt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FaAsWMg58-2",
        "outputId": "45d3842f-e387-400d-daa2-f6f9be03db04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "print(wordnet.synsets(\"computer\"))\n",
        "\n",
        "# Definition and example of the word ‘computer’\n",
        "print(wordnet.synset(\"computer.n.01\").definition())\n",
        "\n",
        "# Examples\n",
        "print(\"Examples:\", wordnet.synset(\"computer.n.01\").examples())\n",
        "\n",
        "# Get Antonyms\n",
        "print(wordnet.lemma('buy.v.01.buy').antonyms())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9uVp4xo5rje",
        "outputId": "5c6d7242-5400-4491-ce92-2838ef8c44c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
            "a machine for performing calculations automatically\n",
            "Examples: []\n",
            "[Lemma('sell.v.01.sell')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 3B: Study lemmas, hyponyms, hypernyms"
      ],
      "metadata": {
        "id": "iRcIi2xO6CRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "JiWpf7ve6gQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "print(wordnet.synsets(\"computer\"))\n",
        "print(wordnet.synset(\"computer.n.01\").lemma_names())  # all lemmas for each synset.\n",
        "for e in wordnet.synsets(\"computer\"):\n",
        "    print(f'{e} --> {e.lemma_names()}')\n",
        "\n",
        "# print all lemmas for a given synset\n",
        "print(wordnet.synset('computer.n.01').lemmas())\n",
        "\n",
        "# get the synset corresponding to lemma\n",
        "print(wordnet.lemma('computer.n.01.computing_device').synset())\n",
        "\n",
        "# Get the name of the lemma\n",
        "print(wordnet.lemma('computer.n.01.computing_device').name())\n",
        "\n",
        "# Hyponyms give abstract concepts of the word that are much more specific\n",
        "# the list of hyponyms words of the computer\n",
        "\n",
        "syn = wordnet.synset('computer.n.01')\n",
        "print(syn.hyponyms())\n",
        "print([lemma.name() for synset in syn.hyponyms() for lemma in synset.lemmas()])\n",
        "\n",
        "# the semantic similarity in WordNet\n",
        "vehicle = wordnet.synset('vehicle.n.01')\n",
        "car = wordnet.synset('car.n.01')\n",
        "\n",
        "print(car.lowest_common_hypernyms(vehicle))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHRlV9B06F5S",
        "outputId": "945d1eaf-2c66-4e41-a480-dfb8075999bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
            "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
            "Synset('computer.n.01') --> ['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
            "Synset('calculator.n.01') --> ['calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n",
            "[Lemma('computer.n.01.computer'), Lemma('computer.n.01.computing_machine'), Lemma('computer.n.01.computing_device'), Lemma('computer.n.01.data_processor'), Lemma('computer.n.01.electronic_computer'), Lemma('computer.n.01.information_processing_system')]\n",
            "Synset('computer.n.01')\n",
            "computing_device\n",
            "[Synset('analog_computer.n.01'), Synset('digital_computer.n.01'), Synset('home_computer.n.01'), Synset('node.n.08'), Synset('number_cruncher.n.02'), Synset('pari-mutuel_machine.n.01'), Synset('predictor.n.03'), Synset('server.n.03'), Synset('turing_machine.n.01'), Synset('web_site.n.01')]\n",
            "['analog_computer', 'analogue_computer', 'digital_computer', 'home_computer', 'node', 'client', 'guest', 'number_cruncher', 'pari-mutuel_machine', 'totalizer', 'totaliser', 'totalizator', 'totalisator', 'predictor', 'server', 'host', 'Turing_machine', 'web_site', 'website', 'internet_site', 'site']\n",
            "[Synset('vehicle.n.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 3C : Write a program using python to find synonym and antonym of word \"active\" using Wordnet."
      ],
      "metadata": {
        "id": "10khkQQ36Rri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "wAqiOsVW6hp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "print(wordnet.synsets(\"active\"))\n",
        "\n",
        "print(wordnet.lemma('active.a.01.active').antonyms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybr0GnwP6WXH",
        "outputId": "3b600448-08d1-4242-fb54-808524b352dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('active_agent.n.01'), Synset('active_voice.n.01'), Synset('active.n.03'), Synset('active.a.01'), Synset('active.s.02'), Synset('active.a.03'), Synset('active.s.04'), Synset('active.a.05'), Synset('active.a.06'), Synset('active.a.07'), Synset('active.s.08'), Synset('active.a.09'), Synset('active.a.10'), Synset('active.a.11'), Synset('active.a.12'), Synset('active.a.13'), Synset('active.a.14')]\n",
            "[Lemma('inactive.a.02.inactive')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 3D : Write a program using python to find synonym and antonym of word \"active\" using Wordnet."
      ],
      "metadata": {
        "id": "_46m93u_6j99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "vCvVGm716trA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "print(wordnet.synsets(\"active\"))\n",
        "\n",
        "print(wordnet.lemma('active.a.01.active').antonyms())\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "syn1 = wordnet.synsets('football')\n",
        "syn2 = wordnet.synsets('soccer')\n",
        "\n",
        "# A word may have multiple synsets, so need to compare each synset of word1 with synset of word2\n",
        "for s1 in syn1:\n",
        "    for s2 in syn2:\n",
        "        print(\"Path similarity of: \")\n",
        "        print(s1, '(', s1.pos(), ')', '[', s1.definition(), ']')\n",
        "        print(s2, '(', s2.pos(), ')', '[', s2.definition(), ']')\n",
        "        print(\" is\", s1.path_similarity(s2))\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "ovNEpe776s39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 3E : Handling stopword:"
      ],
      "metadata": {
        "id": "cpZJoiPk6wuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i)\tUsing nltk Adding or Removing Stop Words in NLTK's Default Stop Word List"
      ],
      "metadata": {
        "id": "Ru9RYY6u7MPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
        "\n",
        "text_tokens = word_tokenize(text)\n",
        "\n",
        "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "\n",
        "print(tokens_without_sw)\n",
        "\n",
        "# Add the word 'play' to the NLTK stop word collection\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.append('play')\n",
        "\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
        "print(tokens_without_sw)\n",
        "\n",
        "# Remove ‘not’ from stop word collection\n",
        "all_stopwords.remove('not')\n",
        "\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
        "print(tokens_without_sw)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejl2dQJW7Oo8",
        "outputId": "64fd3138-5598-400d-d20b-85195a0ebddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Yashesh', 'likes', 'play', 'football', ',', 'fond', 'tennis', '.']\n",
            "['Yashesh', 'likes', 'football', ',', 'however', 'fond', 'tennis', '.']\n",
            "['Yashesh', 'likes', 'football', ',', 'however', 'not', 'fond', 'tennis', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ii)\tUsing Gensim Adding and Removing Stop Words in Default Gensim Stop Words List"
      ],
      "metadata": {
        "id": "gVN31TBw-FGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
        "filtered_sentence = remove_stopwords(text)\n",
        "\n",
        "print(filtered_sentence)\n",
        "\n",
        "import gensim\n",
        "all_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
        "print(all_stopwords)\n",
        "\n",
        "# Add 'likes' and 'play' to the list of stop words in Gensim\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "all_stopwords_gensim = STOPWORDS.union(set(['likes', 'play']))\n",
        "\n",
        "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords_gensim]\n",
        "\n",
        "print(tokens_without_sw)\n",
        "\n",
        "# Remove the word \"not\" from the set of stop words in Gensim\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "all_stopwords_gensim = STOPWORDS\n",
        "sw_list = {\"not\"}\n",
        "all_stopwords_gensim = all_stopwords_gensim.difference(sw_list)\n",
        "\n",
        "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords_gensim]\n",
        "\n",
        "print(tokens_without_sw)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PcDFTqm-HeE",
        "outputId": "d98b3ea1-42e2-4f99-8d42-78dedc723d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yashesh likes play football, fond tennis.\n",
            "frozenset({'whose', 'cannot', 'becoming', 'next', 'without', 'either', 'noone', 'this', 'fill', 'by', 'in', 'had', 'whereupon', 'those', 'have', 'due', 'found', 'about', 'un', 'sometimes', 'seem', 'move', 'out', 'along', 'moreover', 'thereby', 'own', 'bill', 'mill', 'into', 'same', 'keep', 'former', 'also', 'more', 'indeed', 'thru', 'these', 'below', 'somehow', 'did', 'whereafter', 'ours', 'they', 'alone', 'thus', 'on', 'a', 'hence', 'front', 'would', 'fire', 'nine', 'hereafter', 'except', 'his', 'still', 'really', 'therein', 'so', 'therefore', 'but', 'hereupon', 'herself', 'across', 'thin', 'were', 'system', 'latterly', 'amoungst', 'been', 'off', 'kg', 'anywhere', 'ie', 'between', 'fifty', 'back', 'at', 'my', 'via', 'who', 'almost', 'very', 'your', 'then', 'toward', 'hasnt', 'yet', 'together', 'whence', 'until', 'might', 'rather', 'thereupon', 'sometime', 'sixty', 'or', 'whereby', 'every', 'all', 'through', 'you', 'twelve', 'thick', 'see', 'always', 'several', 'that', 'part', 'couldnt', 'thence', 'although', 'wherever', 'namely', 'me', 'doing', 'km', 'beyond', 'another', 'from', 'now', 'became', 'much', 'yourself', 'elsewhere', 'whatever', 'most', 'mostly', 'thereafter', 'be', 'anything', 'can', 'behind', 'themselves', 'say', 'someone', 'an', 'fifteen', 'we', 'being', 'even', 'once', 'beforehand', 'twenty', 'formerly', 'the', 'per', 'before', 'already', 'seemed', 'are', 'get', 'quite', 'each', 'hereby', 'any', 'doesn', 'neither', 'afterwards', 'do', 'nowhere', 'where', 'among', 'is', 'yours', 'has', 'there', 'hers', 'detail', 'above', 'many', 'onto', 'why', 'because', 'to', 'two', 'against', 'whoever', 'how', 'here', 'will', 'whom', 'few', 'for', 'everywhere', 'eleven', 'while', 'put', 'just', 'us', 'go', 'though', 'didn', 'am', 'however', 'less', 'describe', 'co', 'too', 'nobody', 'himself', 'was', 'of', 'de', 'take', 'name', 'latter', 'everyone', 'both', 'whenever', 'whither', 'herein', 'often', 'empty', 'she', 'some', 'around', 'computer', 'eight', 'none', 'should', 'regarding', 'don', 'them', 'one', 'mine', 'show', 'than', 'amount', 'seeming', 'three', 'six', 'since', 'anyway', 'itself', 'myself', 'everything', 'does', 'him', 'up', 'such', 'ourselves', 'whole', 'last', 'yourselves', 're', 'con', 'under', 'after', 'top', 'further', 'during', 'within', 'and', 'forty', 'other', 'if', 'whether', 'made', 'otherwise', 'whereas', 'amongst', 'besides', 'again', 'full', 'towards', 'eg', 'sincere', 'else', 'find', 'give', 'unless', 'as', 'become', 'when', 'only', 'first', 'five', 'seems', 'not', 'ten', 'nothing', 'anyhow', 'its', 'nor', 'etc', 'her', 'anyone', 'could', 'done', 'inc', 'used', 'serious', 'bottom', 'ever', 'others', 'interest', 'it', 'perhaps', 'well', 'down', 'our', 'no', 'may', 'their', 'using', 'third', 'he', 'with', 'meanwhile', 'make', 'which', 'cry', 'becomes', 'wherein', 'ltd', 'hundred', 'four', 'over', 'somewhere', 'least', 'beside', 'something', 'nevertheless', 'call', 'upon', 'what', 'cant', 'must', 'enough', 'side', 'throughout', 'never', 'i', 'please', 'various'})\n",
            "['Yashesh', 'football', ',', 'fond', 'tennis', '.']\n",
            "['Yashesh', 'likes', 'play', 'football', ',', 'not', 'fond', 'tennis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iii) Using Spacy Adding and Removing Stop Words in Default Spacy Stop Words List"
      ],
      "metadata": {
        "id": "HB0cwmGu-mbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "# Get the default stop words from Spacy\n",
        "all_stopwords = sp.Defaults.stop_words\n",
        "# Add \"play\" to the stop words\n",
        "all_stopwords.add(\"play\")\n",
        "\n",
        "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
        "print(tokens_without_sw)\n",
        "\n",
        "# Remove 'not' from the stop word collection\n",
        "all_stopwords.remove('not')\n",
        "\n",
        "# Reapply the stop word removal\n",
        "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
        "print(tokens_without_sw)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GiW3QFf-sQH",
        "outputId": "e898a7b1-3b93-4e55-f95e-d739592dc114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Yashesh', 'likes', 'football', ',', 'fond', 'tennis', '.']\n",
            "['Yashesh', 'likes', 'football', ',', 'not', 'fond', 'tennis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 4 : Text Tokenization"
      ],
      "metadata": {
        "id": "SlqkZnFuQfcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## a. Tokenization using Python’s split() function\n",
        "\n",
        "text = \"\"\" This tool is an a beta stage. Alexa developers can use Get Metrics API to seamlessly\n",
        "analyse metric. It also supports custom skill model, prebuilt Flash Briefingmodel, and the\n",
        "Smart Home Skill API. You can use this tool for creation of monitors, alarms, and dashboards\n",
        "that spotlight changes. The release of these three tools will enable developers to create visual\n",
        "rich skills for Alexa devices with screens. Amazon describes these tools as the collection of\n",
        "tech and tools for creating visually rich and interactive voice experiences. \"\"\"\n",
        "data = text.split('.')\n",
        "for i in data:\n",
        "  print (i)\n",
        "\n",
        "## b. Tokenization using Regular Expressions (RegEx)\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "# Create a reference variable for RegexpTokenizer\n",
        "RegexpTokenizer_tk = RegexpTokenizer('\\s+', gaps = True)\n",
        "# Create a string input\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "# Use tokenize method\n",
        "tokens = RegexpTokenizer_tk.tokenize(str)\n",
        "print(tokens)\n",
        "\n",
        "\n",
        "## c.  Tokenization using NLTK\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "# Create a reference variable for RegexpTokenizer\n",
        "RegexpTokenizer_tk = RegexpTokenizer('\\s+', gaps = True)\n",
        "# Create a string input\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "# Use tokenize method\n",
        "tokens = RegexpTokenizer_tk.tokenize(str)\n",
        "print(tokens)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Create a string input\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "# Use tokenize method\n",
        "print(word_tokenize(str))\n",
        "\n",
        "\n",
        "## d. Tokenization using the spaCy library\n",
        "import spacy\n",
        "nlp = spacy.blank(\"en\")\n",
        "# Create a string input\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "# Create an instance of document;\n",
        "# doc object is a container for a sequence of Token\n",
        "doc = nlp(str)\n",
        "# Read the words; Print the words#\n",
        "words = [word.text for word in doc]\n",
        "print(words)\n",
        "\n",
        "\n",
        "## e. Tokenization using Keras\n",
        "#pip install keras\n",
        "#pip install tensorflow\n",
        "import keras\n",
        "from keras.preprocessing.text import text_to_word_sequence # Createa string input\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "# tokenizing the text\n",
        "tokens = text_to_word_sequence(str)\n",
        "print(tokens)\n",
        "\n",
        "\n",
        "## f. c. Tokenization using Gensim\n",
        "#pip install gensim\n",
        "from gensim.utils import tokenize # Create a string input\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "# tokenizing the text\n",
        "list(tokenize(str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8IhtAogQfO8",
        "outputId": "a1915a38-341f-4b11-8a80-909023a1a884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This tool is an a beta stage\n",
            " Alexa developers can use Get Metrics API to seamlessly\n",
            "analyse metric\n",
            " It also supports custom skill model, prebuilt Flash Briefingmodel, and the\n",
            "Smart Home Skill API\n",
            " You can use this tool for creation of monitors, alarms, and dashboards\n",
            "that spotlight changes\n",
            " The release of these three tools will enable developers to create visual\n",
            "rich skills for Alexa devices with screens\n",
            " Amazon describes these tools as the collection of\n",
            "tech and tools for creating visually rich and interactive voice experiences\n",
            " \n",
            "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n",
            "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n",
            "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n",
            "['i', 'love', 'to', 'study', 'natural', 'language', 'processing', 'in', 'python']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'love',\n",
              " 'to',\n",
              " 'study',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'in',\n",
              " 'Python']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 5 : Import NLP Libraries for Indian Languages and perform:"
      ],
      "metadata": {
        "id": "uf6X3NeWWx90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "QyScMfo1W7CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tornado==4.5.3"
      ],
      "metadata": {
        "id": "d_Y_vg0AW-fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inltk"
      ],
      "metadata": {
        "id": "q2OeMNUTW88D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the setup('hi') call after importing the necessary modules.\n",
        "from inltk.inltk import tokenize\n",
        "from collections import abc, Counter, defaultdict, namedtuple, OrderedDict\n",
        "from collections.abc import Iterable\n",
        "from inltk.inltk import setup\n",
        "setup('hi')\n",
        "hindi_text = \"\"\" र क त क भ ष खन बह त त लच ह ।\"\"\"\n",
        "# tokenize(input text, language code)\n",
        "tokenize(hindi_text, \"hi\")\n",
        "setup('hi')"
      ],
      "metadata": {
        "id": "HLMADtRFYSVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##.....Generate similar sentences from a given Hindi text input\n",
        "from inltk.inltk import setup\n",
        "setup('hi')\n",
        "from inltk.inltk import get_similar_sentences\n",
        "# get similar sentences to the one given in hindi\n",
        "output = get_similar_sentences('म आज ब त ख¸श ह ', 5,'hi')\n",
        "print(output)"
      ],
      "metadata": {
        "id": "gnmfDbyCWyi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##.....Identify the Indian language of a text\n",
        "\n",
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install inltk\n",
        "!pip install tornado==4.5.3\n",
        "from inltk.inltk import setup\n",
        "setup('gu')\n",
        "from inltk.inltk import identify_language\n",
        "#Identify the Lnaguage of given text\n",
        "identify_language(' ')\n"
      ],
      "metadata": {
        "id": "UgbrblQOYXBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 6 : Illustrate part of speech tagging."
      ],
      "metadata": {
        "id": "61WWzb3CclPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A) Part of speech Tagging and chunking of user defined text."
      ],
      "metadata": {
        "id": "6EtnkurmcruG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import tag\n",
        "from nltk import chunk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "para = \"Hello! My name is Beena Kapadia. Today you'll be learning NLTK.\"\n",
        "sents = tokenize.sent_tokenize(para)\n",
        "print(\"\\nsentence tokenization\\n===================\\n\",sents)\n",
        "print(\"\\nword tokenization\\n===================\\n\")\n",
        "for index in range(len(sents)):\n",
        "  words = tokenize.word_tokenize(sents[index])\n",
        "  print(words)\n",
        "tagged_words = []\n",
        "for index in range(len(sents)):\n",
        "  tagged_words.append(tag.pos_tag(words))\n",
        "  print(\"\\nPOS Tagging\\n===========\\n\",tagged_words)\n",
        "tree = []\n",
        "for index in range(len(sents)):\n",
        "  tree.append(chunk.ne_chunk(tagged_words[index]))\n",
        "  print(\"\\nchunking\\n========\\n\")\n",
        "  print(tree)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIUwW2_pclla",
        "outputId": "2f3c8a0d-80fd-4c91-8948-296a9a5171fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "sentence tokenization\n",
            "===================\n",
            " ['Hello!', 'My name is Beena Kapadia.', \"Today you'll be learning NLTK.\"]\n",
            "\n",
            "word tokenization\n",
            "===================\n",
            "\n",
            "['Hello', '!']\n",
            "['My', 'name', 'is', 'Beena', 'Kapadia', '.']\n",
            "['Today', 'you', \"'ll\", 'be', 'learning', 'NLTK', '.']\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')]]\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')], [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')]]\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')], [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')], [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLTK', 'NNP'), ('.', '.')]]\n",
            "\n",
            "chunking\n",
            "========\n",
            "\n",
            "[Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')])]\n",
            "\n",
            "chunking\n",
            "========\n",
            "\n",
            "[Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')]), Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')])]\n",
            "\n",
            "chunking\n",
            "========\n",
            "\n",
            "[Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')]), Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')]), Tree('S', [('Today', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP')]), ('.', '.')])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#..B...Named Entity recognition using user defined text"
      ],
      "metadata": {
        "id": "1e8PVuDqdKP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_3TYUdPdLJ_",
        "outputId": "399c0889-3588-40ef-e984-4d7a26d8b4ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCI6XKfCdMsd",
        "outputId": "67c0c76b-a3bc-4369-bba9-8d8e724cff8d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. ―I can tell you very senior CEOs of major American \"\n",
        "\"car companies would shake my hand and turn away because I wasn‘t worth talking to,‖ said Thrun, in an interview with Recode earlier \" \"this week.\")\n",
        "doc = nlp(text)\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\",[token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTK3p7_sdGeu",
        "outputId": "ecccc867-ff48-4619-99cc-effab4f2f873"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', '―I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'wasn‘t', 'to,‖', 'Thrun', 'an interview', 'Recode']\n",
            "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C) Named Entity recognition with diagram using NLTK corpus – treebank."
      ],
      "metadata": {
        "id": "qwuFDMcgeEtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaK7zXxNkSTS",
        "outputId": "e012a8bf-2dbd-454c-b545-f9e612fc31cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.4.0-py3-none-any.whl (23 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.4.0 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank_chunk\n",
        "treebank_chunk.tagged_sents()[0]\n",
        "treebank_chunk.chunked_sents()[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "_Sxrlrh2eCQn",
        "outputId": "8e19886f-1129-4cfe-bf8d-ac4ffdd21dda"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', [('Pierre', 'NNP'), ('Vinken', 'NNP')]), (',', ','), Tree('NP', [('61', 'CD'), ('years', 'NNS')]), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), Tree('NP', [('the', 'DT'), ('board', 'NN')]), ('as', 'IN'), Tree('NP', [('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD')]), ('.', '.')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,856.0,168.0\" width=\"856px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"14.9533%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"50%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Pierre</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"50%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Vinken</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.47664%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.80374%\" x=\"14.9533%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.3551%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.2804%\" x=\"17.757%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"36.3636%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">61</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.1818%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"63.6364%\" x=\"36.3636%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">years</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.1818%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.8972%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.6729%\" x=\"28.0374%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">old</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.3738%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.80374%\" x=\"32.7103%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.1121%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.60748%\" x=\"35.514%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">will</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">MD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"38.3178%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.60748%\" x=\"41.1215%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">join</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"43.9252%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"11.215%\" x=\"46.729%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"41.6667%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.8333%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"58.3333%\" x=\"41.6667%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">board</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.8333%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.3364%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.73832%\" x=\"57.9439%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">as</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.8131%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"35.514%\" x=\"61.6822%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"10.5263%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.26316%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"36.8421%\" x=\"10.5263%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">nonexecutive</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.9474%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"26.3158%\" x=\"47.3684%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">director</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.5263%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"15.7895%\" x=\"73.6842%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Nov.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.5789%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.5263%\" x=\"89.4737%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">29</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.7368%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.4393%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.80374%\" x=\"97.1963%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.5981%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 7 :"
      ],
      "metadata": {
        "id": "WGEA3QQfQ12I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a)\tDefine grammar using nltk. Analyze a sentence using the same."
      ],
      "metadata": {
        "id": "4HALUXtpR9te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n"
      ],
      "metadata": {
        "id": "QCh84zYuRAyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the grammar\n",
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "S -> VP\n",
        "VP -> VP NP\n",
        "NP -> Det NP\n",
        "Det -> 'that'\n",
        "NP -> 'singularNoun'\n",
        "NP -> 'flight'\n",
        "VP -> 'Book'\"\"\")\n",
        "\n",
        "sentence = \"Book that flight\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "all_tokens = tokenize.word_tokenize(sentence)\n",
        "print(all_tokens)\n",
        "\n",
        "# Create a parser\n",
        "parser = nltk.ChartParser(grammar1)\n",
        "\n",
        "# Parse the sentence and print the tree structure\n",
        "for tree in parser.parse(all_tokens):\n",
        "    print(tree)\n",
        "    # Plot the tree\n",
        "    tree.pretty_print()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhBU7bdJRxKi",
        "outputId": "59f09160-dab7-445e-a260-713019ba10d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Book', 'that', 'flight']\n",
            "(S (VP (VP Book) (NP (Det that) (NP flight))))\n",
            "      S             \n",
            "      |              \n",
            "      VP            \n",
            "  ____|____          \n",
            " |         NP       \n",
            " |     ____|____     \n",
            " VP  Det        NP  \n",
            " |    |         |    \n",
            "Book that     flight\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b)\tAccept the input string with Regular expression of Finite Automaton: 101+."
      ],
      "metadata": {
        "id": "s2B4WZEbUqOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FA(s):\n",
        "  if len(s)<3:\n",
        "    return \"Rejected\"\n",
        "  if s[0]=='1':\n",
        "    if s[1]=='0':\n",
        "      if s[2]=='1':\n",
        "        for i in range(3,len(s)):\n",
        "          if s[i]!='1':\n",
        "            return \"Rejected\"\n",
        "        return \"Accepted\"\n",
        "  return \"Rejected\"\n",
        "\n",
        "inputs = ['1','10101','101','10111','01010','100','','10111101','1011111']\n",
        "for i in inputs:\n",
        "  print(FA(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuFGNV1HUu3x",
        "outputId": "0222ab6d-a8f4-467a-e3b3-00e21c4ce88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rejected\n",
            "Rejected\n",
            "Accepted\n",
            "Accepted\n",
            "Rejected\n",
            "Rejected\n",
            "Rejected\n",
            "Rejected\n",
            "Accepted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Accept the input string with Regular expression of FA: (a+b)*bba."
      ],
      "metadata": {
        "id": "IKmtbWaJUyd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FA(s):\n",
        "    size = 0\n",
        "    for i in s:\n",
        "        if i == 'a' or i == 'b':\n",
        "            size += 1\n",
        "        else:\n",
        "            return \"Rejected\"\n",
        "    if size >= 3:\n",
        "        # check the last 3 elements\n",
        "        if s[size-3] == 'b' and s[size-2] == 'b' and s[size-1] == 'a':\n",
        "            return \"Accepted\"\n",
        "        else:\n",
        "            return \"Rejected\"\n",
        "    else:\n",
        "        return \"Rejected\"\n",
        "inputs = ['bba', 'ababbba', 'abba', 'abb', 'baba', 'bbb', '']\n",
        "for i in inputs:\n",
        "    print(FA(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIMWgxK_U3A-",
        "outputId": "d13dc316-5b49-4624-c4a9-ed014586eb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accepted\n",
            "Accepted\n",
            "Accepted\n",
            "Rejected\n",
            "Rejected\n",
            "Rejected\n",
            "Rejected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d)\tImplementation of Deductive Chart Parsing using context free grammar and a given sentence."
      ],
      "metadata": {
        "id": "vFSeoxlDU7Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('punkt')\n",
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "S -> VP\n",
        "VP -> VP NP | 'Book'\n",
        "NP -> Det NP | 'flight'\n",
        "Det -> 'that'\n",
        "Det -> 'a'\n",
        "NP -> 'flight'\n",
        "\"\"\")\n",
        "sentence = \"Book that flight\"\n",
        "all_tokens = tokenize.word_tokenize(sentence)\n",
        "print(all_tokens)\n",
        "parser = nltk.ChartParser(grammar1)\n",
        "for tree in parser.parse(all_tokens):\n",
        "    tree.pretty_print()\n",
        "    print(tree)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    #tree.draw()\n",
        "    plt.savefig(tree_filename)\n",
        "    plt.close()\n",
        "    #tree.draw()  # Uncomment this if you have a graphical display available\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('punkt')\n",
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "VP -> V NP | VP PP\n",
        "PP -> P NP\n",
        "NP -> Det N | Det N PP | 'I'\n",
        "Det -> 'a' | 'my'\n",
        "N -> 'bird' | 'balcony'\n",
        "V -> 'saw'\n",
        "P -> 'in'\n",
        "\"\"\")\n",
        "sentence = \"I saw a bird in my balcony\"\n",
        "all_tokens = tokenize.word_tokenize(sentence)\n",
        "parser = nltk.ChartParser(grammar1)\n",
        "for tree in parser.parse(all_tokens):\n",
        "    tree_filename = \"parse_tree.png\"\n",
        "    tree.pretty_print()\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    #tree.draw()\n",
        "    plt.savefig(tree_filename)\n",
        "    plt.close()\n",
        "    print(\"Parse tree saved as:\", tree_filename)"
      ],
      "metadata": {
        "id": "YR8wTSuCWoGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 8 : Study PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer Study WordNetLemmatizer"
      ],
      "metadata": {
        "id": "SaTFgByzQX9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PorterStemmer\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "print(word_stemmer.stem('writing'))\n",
        "\n",
        "#LancasterStemmer\n",
        "import nltk\n",
        "from nltk.stem import LancasterStemmer\n",
        "Lanc_stemmer = LancasterStemmer()\n",
        "print(Lanc_stemmer.stem('writing'))\n",
        "\n",
        "#RegexpStemmer\n",
        "import nltk\n",
        "from nltk.stem import RegexpStemmer\n",
        "Reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "print(Reg_stemmer.stem('writing'))\n",
        "\n",
        "#SnowballStemmer\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "english_stemmer = SnowballStemmer('english')\n",
        "print(english_stemmer.stem ('writing'))\n",
        "\n",
        "#WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"word :\\tlemma\")\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
      ],
      "metadata": {
        "id": "yJmJC2eSQYa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 9 : Implement Naive Bayes classifier"
      ],
      "metadata": {
        "id": "NR9vF_u7PgvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2anODYrPmsw",
        "outputId": "b0b2cd12-7047-437c-d1cc-1e64b8504a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dJc0YX1PqQZ",
        "outputId": "e2b18130-0309-4ef2-a8f1-7482da7e320d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec9ZQN4wPsM9",
        "outputId": "850fad3a-b9d0-48e2-f72d-edad1533d3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# select any spam.csv dataset from github.\n",
        "sms_data = pd.read_csv(\"spamdata.csv\", encoding='latin-1')\n",
        "# using below code line check the column names.\n",
        "print(sms_data.columns)\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemming = PorterStemmer()\n",
        "corpus = []\n",
        "for i in range (0,len(sms_data)):\n",
        "  s1 = re.sub('[^a-zA-Z]',repl = ' ',string = sms_data['Message'][i])  # put 2nd column name here.\n",
        "  s1.lower()\n",
        "  s1 = s1.split()\n",
        "  s1 = [stemming.stem(word) for word in s1 if word not in\n",
        "        set(stopwords.words('english'))]\n",
        "  s1 = ' '.join(s1)\n",
        "  corpus.append(s1)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "countvectorizer =CountVectorizer()\n",
        "x = countvectorizer.fit_transform(corpus).toarray()\n",
        "print(x)\n",
        "y = sms_data['Category'].values # put 1st column name here.\n",
        "print(y)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,\n",
        "stratify=y,random_state=2)\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "multinomialnb = MultinomialNB()\n",
        "multinomialnb.fit(x_train,y_train)\n",
        "y_pred = multinomialnb.predict(x_test)\n",
        "print(y_pred)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"accuracy_score: \",accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "PStyq_amPf3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 10 A : Speech Tagging"
      ],
      "metadata": {
        "id": "toQU2BBS_5Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i) Speech tagging using spacy"
      ],
      "metadata": {
        "id": "b1CFNFIYAnYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "sen = sp(u\"I like to play football. I hated it in my childhood though\")\n",
        "print(sen.text)\n",
        "print(sen[7].pos_)\n",
        "print(sen[7].tag_)\n",
        "print(spacy.explain(sen[7].tag_))\n",
        "\n",
        "for word in sen:\n",
        "    print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n",
        "\n",
        "sen = sp(u'Can you google it?')\n",
        "word = sen[2]\n",
        "print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n",
        "\n",
        "sen = sp(u'Can you search it on google?')\n",
        "word = sen[5]\n",
        "print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n",
        "\n",
        "# Finding the Number of POS Tags\n",
        "sen = sp(u\"I like to play football. I hated it in my childhood though\")\n",
        "num_pos = sen.count_by(spacy.attrs.POS)\n",
        "\n",
        "for k,v in sorted(num_pos.items()):\n",
        "    print(f'{k}. {sen.vocab[k].text:{8}}: {v}')\n",
        "\n",
        "# Visualizing Parts of Speech Tags\n",
        "from spacy import displacy\n",
        "\n",
        "sen = sp(u\"I like to play football. I hated it in my childhood though\")\n",
        "displacy.serve(sen, style='dep', options={'distance': 120})\n"
      ],
      "metadata": {
        "id": "WZjvOMUj_sTm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "345f6146-350b-476a-8d4a-a83d7ab382e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like to play football. I hated it in my childhood though\n",
            "VERB\n",
            "VBD\n",
            "verb, past tense\n",
            "I            PRON       PRP      pronoun, personal\n",
            "like         VERB       VBP      verb, non-3rd person singular present\n",
            "to           PART       TO       infinitival \"to\"\n",
            "play         VERB       VB       verb, base form\n",
            "football     NOUN       NN       noun, singular or mass\n",
            ".            PUNCT      .        punctuation mark, sentence closer\n",
            "I            PRON       PRP      pronoun, personal\n",
            "hated        VERB       VBD      verb, past tense\n",
            "it           PRON       PRP      pronoun, personal\n",
            "in           ADP        IN       conjunction, subordinating or preposition\n",
            "my           PRON       PRP$     pronoun, possessive\n",
            "childhood    NOUN       NN       noun, singular or mass\n",
            "though       ADV        RB       adverb\n",
            "google       VERB       VB       verb, base form\n",
            "google       PROPN      NNP      noun, proper singular\n",
            "85. ADP     : 1\n",
            "86. ADV     : 1\n",
            "92. NOUN    : 2\n",
            "94. PART    : 1\n",
            "95. PRON    : 4\n",
            "97. PUNCT   : 1\n",
            "100. VERB    : 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/displacy/__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
            "  warnings.warn(Warnings.W011)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"32a088bb6a5e4d1f84d85df481478519-0\" class=\"displacy\" width=\"1490\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">like</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">play</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">football.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">hated</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">it</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">my</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">childhood</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1370\">though</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1370\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-32a088bb6a5e4d1f84d85df481478519-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,122.0 160.0,122.0 160.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-32a088bb6a5e4d1f84d85df481478519-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-32a088bb6a5e4d1f84d85df481478519-0-1\" stroke-width=\"2px\" d=\"M310,182.0 C310,122.0 400.0,122.0 400.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-32a088bb6a5e4d1f84d85df481478519-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M310,184.0 L302,172.0 318,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-32a088bb6a5e4d1f84d85df481478519-0-2\" stroke-width=\"2px\" d=\"M190,182.0 C190,62.0 405.0,62.0 405.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-32a088bb6a5e4d1f84d85df481478519-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M405.0,184.0 L413.0,172.0 397.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-32a088bb6a5e4d1f84d85df481478519-0-3\" stroke-width=\"2px\" d=\"M430,182.0 C430,122.0 520.0,122.0 520.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-32a088bb6a5e4d1f84d85df481478519-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M520.0,184.0 L528.0,172.0 512.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-32a088bb6a5e4d1f84d85df481478519-0-4\" stroke-width=\"2px\" d=\"M670,182.0 C670,122.0 760.0,122.0 760.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-32a088bb6a5e4d1f84d85df481478519-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M670,184.0 L662,172.0 678,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-32a088bb6a5e4d1f84d85df481478519-0-5\" stroke-width=\"2px\" d=\"M790,182.0 C790,122.0 880.0,122.0 880.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-32a088bb6a5e4d1f84d85df481478519-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M880.0,184.0 L888.0,172.0 872.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-32a088bb6a5e4d1f84d85df481478519-0-6\" stroke-width=\"2px\" d=\"M790,182.0 C790,62.0 1005.0,62.0 1005.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-32a088bb6a5e4d1f84d85df481478519-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1005.0,184.0 L1013.0,172.0 997.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-32a088bb6a5e4d1f84d85df481478519-0-7\" stroke-width=\"2px\" d=\"M1150,182.0 C1150,122.0 1240.0,122.0 1240.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-32a088bb6a5e4d1f84d85df481478519-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1150,184.0 L1142,172.0 1158,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-32a088bb6a5e4d1f84d85df481478519-0-8\" stroke-width=\"2px\" d=\"M1030,182.0 C1030,62.0 1245.0,62.0 1245.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-32a088bb6a5e4d1f84d85df481478519-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1245.0,184.0 L1253.0,172.0 1237.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-32a088bb6a5e4d1f84d85df481478519-0-9\" stroke-width=\"2px\" d=\"M790,182.0 C790,2.0 1370.0,2.0 1370.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-32a088bb6a5e4d1f84d85df481478519-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1370.0,184.0 L1378.0,172.0 1362.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>\n",
              "</figure>\n",
              "</body>\n",
              "</html></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ii) Speech tagging using nktl"
      ],
      "metadata": {
        "id": "B_hmyz-X_vdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('state_union')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "#create our training and testing data:\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "#train the Punkt tokenizer like:\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "# tokenize:\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "def process_content():\n",
        " try:\n",
        "  for i in tokenized[:2]:\n",
        "    words = nltk.word_tokenize(i)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "    print(tagged)\n",
        " except Exception as e:\n",
        "  print(str(e))\n",
        "process_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqfYxv18AOUH",
        "outputId": "5c0ad541-e0b5-4eb7-b7a4-602e9684b6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
            "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 10 B : Statistical parsin"
      ],
      "metadata": {
        "id": "qWb8TUmPAvYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i)\tUsage of Give and Gave in the Penn Treebank sample"
      ],
      "metadata": {
        "id": "NsV4oV_uA1Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "import nltk.parse.viterbi\n",
        "import nltk.parse.pchart\n",
        "\n",
        "def give(t):\n",
        "    return t.label() == 'VP' and len(t) > 2 and t[1].label() == 'NP' \\\n",
        "        and (t[2].label() == 'PP-DTV' or t[2].label() == 'NP') \\\n",
        "        and ('give' in t[0].leaves() or 'gave' in t[0].leaves())\n",
        "\n",
        "def sent(t):\n",
        "    return ' '.join(token for token in t.leaves() if token[0] not in '*-0')\n",
        "\n",
        "\n",
        "def print_node(t, width):\n",
        "    output = \"%s %s: %s / %s: %s\" % \\\n",
        "        (sent(t[0]), t[1].label(), sent(t[1]), t[2].label(), sent(t[2]))\n",
        "    if len(output) > width:\n",
        "        output = output[:width] + \"...\"\n",
        "    print(output)\n",
        "\n",
        "for tree in nltk.corpus.treebank.parsed_sents():\n",
        "    for t in tree.subtrees(give):\n",
        "        print_node(t, 72)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZkfpQU5A5_Z",
        "outputId": "bc304974-d6bf-4617-865f-f820b5ae4410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gave NP: the chefs / NP: a standing ovation\n",
            "give NP: advertisers / NP: discounts for maintaining or increasing ad sp...\n",
            "give NP: it / PP-DTV: to the politicians\n",
            "gave NP: them / NP: similar help\n",
            "give NP: them / NP: \n",
            "give NP: only French history questions / PP-DTV: to students in a Europe...\n",
            "give NP: federal judges / NP: a raise\n",
            "give NP: consumers / NP: the straight scoop on the U.S. waste crisis\n",
            "gave NP: Mitsui / NP: access to a high-tech medical product\n",
            "give NP: Mitsubishi / NP: a window on the U.S. glass industry\n",
            "give NP: much thought / PP-DTV: to the rates she was receiving , nor to ...\n",
            "give NP: your Foster Savings Institution / NP: the gift of hope and free...\n",
            "give NP: market operators / NP: the authority to suspend trading in futu...\n",
            "gave NP: quick approval / PP-DTV: to $ 3.18 billion in supplemental appr...\n",
            "give NP: the Transportation Department / NP: up to 50 days to review any...\n",
            "give NP: the president / NP: such power\n",
            "give NP: me / NP: the heebie-jeebies\n",
            "give NP: holders / NP: the right , but not the obligation , to buy a cal...\n",
            "gave NP: Mr. Thomas / NP: only a `` qualified '' rating , rather than ``...\n",
            "give NP: the president / NP: line-item veto power\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ii)\tProbabilistic Parser"
      ],
      "metadata": {
        "id": "ZGbYM_cPBNXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import PCFG\n",
        "\n",
        "grammar = PCFG.fromstring('''\n",
        "    NP -> NNS [0.5] | JJ NNS [0.3] | NP CC NP [0.2]\n",
        "    NNS -> \"men\" [0.1] | \"women\" [0.2] | \"children\" [0.3] | NNS CC NNS [0.4]\n",
        "    JJ -> \"old\" [0.4] | \"young\" [0.6]\n",
        "    CC -> \"and\" [0.9] | \"or\" [0.1]\n",
        "''')\n",
        "\n",
        "print(grammar)\n",
        "\n",
        "viterbi_parser = nltk.ViterbiParser(grammar)\n",
        "token = \"old men and women\".split()\n",
        "obj = viterbi_parser.parse(token)\n",
        "\n",
        "print(\"Output: \")\n",
        "for x in obj:\n",
        "    print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQoKy1QmBSIV",
        "outputId": "5e9ec0d0-41c6-49ec-b17c-1706e7be74b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar with 11 productions (start state = NP)\n",
            "    NP -> NNS [0.5]\n",
            "    NP -> JJ NNS [0.3]\n",
            "    NP -> NP CC NP [0.2]\n",
            "    NNS -> 'men' [0.1]\n",
            "    NNS -> 'women' [0.2]\n",
            "    NNS -> 'children' [0.3]\n",
            "    NNS -> NNS CC NNS [0.4]\n",
            "    JJ -> 'old' [0.4]\n",
            "    JJ -> 'young' [0.6]\n",
            "    CC -> 'and' [0.9]\n",
            "    CC -> 'or' [0.1]\n",
            "Output: \n",
            "(NP (JJ old) (NNS (NNS men) (CC and) (NNS women))) (p=0.000864)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 10 C : Malt parsing: Parse a sentence and draw a tree using malt parsing."
      ],
      "metadata": {
        "id": "1XrSOLZwBwWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Java should be installed.\n",
        "# 2) maltparser-1.7.2 zip file should be copied in C:\\Users\\Beena Kapadia\\AppData\\Local\\Programs\\Python\\Python39 folder and should be extracted in the same folder.\n",
        "# 3) engmalt.linear-1.7.mco file should be copied to C:\\Users\\Beena Kapadia\\AppData\\Local\\Programs\\Python\\Python39 folder\n",
        "\n",
        "# Link for engmalt.linear-1.7.mco - https://maltparser.org/mco/english_parser/engmalt.linear-1.7.mco\n",
        "# Link for maltparser-1.7.2 zip http://maltparser.org/dist/maltparser-1.9.2.zip"
      ],
      "metadata": {
        "id": "EMbC-PcJm0Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.parse import malt\n",
        "\n",
        "# Initialize MaltParser\n",
        "mp = malt.MaltParser('maltparser-1.7.2', 'engmalt.linear-1.7.mco')\n",
        "\n",
        "# Parse a sentence\n",
        "parsed_tree = mp.parse_one('I saw a bird from my window.'.split()).tree()\n",
        "\n",
        "# Visualize the parse tree\n",
        "parsed_tree.draw()"
      ],
      "metadata": {
        "id": "EUHqDYR8N459"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 11 A: Multiword Expressions in NLP"
      ],
      "metadata": {
        "id": "MFhP9nFXB2Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "s = '''Good cake cost Rs.1500\\kg in Mumbai. Please buy me one of them.\\n\\nThanks.'''\n",
        "mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')], separator='_')\n",
        "for sent in sent_tokenize(s):\n",
        "  print(mwe.tokenize(word_tokenize(sent)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iuKqAIYN0-w",
        "outputId": "7157baac-e3bc-45f6-ddba-2eedc1a6c8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'cake', 'cost', 'Rs.1500\\\\kg', 'in', 'Mumbai', '.']\n",
            "['Please', 'buy', 'me', 'one', 'of', 'them', '.']\n",
            "['Thanks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 11 B: Normalized Web Distance and Word Similarity"
      ],
      "metadata": {
        "id": "VgpPKVUhOOwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textdistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7hSsh4uOn42",
        "outputId": "12e5d694-e6fc-4929-e979-e71741e8b1cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textdistance\n",
            "  Downloading textdistance-4.6.1-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import textdistance  # pip install textdistance\n",
        "# we will need scikit-learn>=0.21\n",
        "import sklearn  # pip install sklearn\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "texts = [\n",
        "    'Reliance supermarket', 'Reliance hypermarket', 'Reliance', 'Reliance', 'Reliance downtown', 'Relianc market',\n",
        "    'Mumbai', 'Mumbai Hyper', 'Mumbai dxb', 'mumbai airport',\n",
        "    'k.m trading', 'KM Trading', 'KM trade', 'K.M. Trading', 'KM.Trading'\n",
        "]\n",
        "\n",
        "\n",
        "def normalize(text):\n",
        "    \"\"\" Keep only lower-cased text and numbers\"\"\"\n",
        "    return re.sub('[^a-z0-9]+', ' ', text.lower())\n",
        "\n",
        "\n",
        "def group_texts(texts, threshold=0.4):\n",
        "    \"\"\" Replace each text with the representative of its cluster\"\"\"\n",
        "    normalized_texts = np.array([normalize(text) for text in texts])\n",
        "    distances = 1 - np.array([\n",
        "        [textdistance.jaro_winkler(one, another) for one in normalized_texts]\n",
        "        for another in normalized_texts\n",
        "    ])\n",
        "    clustering = AgglomerativeClustering(\n",
        "        distance_threshold=threshold,  # this parameter needs to be tuned carefully\n",
        "        affinity=\"precomputed\", linkage=\"complete\", n_clusters=None\n",
        "    ).fit(distances)\n",
        "    centers = dict()\n",
        "    for cluster_id in set(clustering.labels_):\n",
        "        index = clustering.labels_ == cluster_id\n",
        "        centrality = distances[:, index][index].sum(axis=1)\n",
        "        centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n",
        "    return [centers[i] for i in clustering.labels_]\n",
        "\n",
        "\n",
        "print(group_texts(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhXH2xblOTVQ",
        "outputId": "24fecca1-95e2-4139-a3cb-ec46bede7122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'mumbai', 'mumbai', 'mumbai', 'mumbai', 'km trading', 'km trading', 'km trading', 'km trading', 'km trading']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical 11 C: Word Sense Disambiguation"
      ],
      "metadata": {
        "id": "652_btq2O0Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "def get_first_sense(word, pos=None):\n",
        " if pos:\n",
        "  synsets = wn.synsets(word,pos)\n",
        " else:\n",
        "  synsets = wn.synsets(word)\n",
        " return synsets[0]\n",
        "best_synset = get_first_sense('bank')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
        "best_synset = get_first_sense('set','n')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
        "best_synset = get_first_sense('set','v')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwRQk7XRO3KI",
        "outputId": "3dd969a4-43eb-44ee-a108-a427e1752462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Synset.name of Synset('bank.n.01')>: <bound method Synset.definition of Synset('bank.n.01')>\n",
            "<bound method Synset.name of Synset('set.n.01')>: <bound method Synset.definition of Synset('set.n.01')>\n",
            "<bound method Synset.name of Synset('put.v.01')>: <bound method Synset.definition of Synset('put.v.01')>\n"
          ]
        }
      ]
    }
  ]
}